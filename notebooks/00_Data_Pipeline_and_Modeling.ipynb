{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d392e4b",
   "metadata": {},
   "source": [
    "# 🚗 Complete Traffic Forecasting Pipeline - HCMC\n",
    "\n",
    "**All-in-One Workflow**: Download → Preprocess → EDA → Feature Engineering → Model Training\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Pipeline Overview\n",
    "\n",
    "| Step | Description | Optional |\n",
    "|------|-------------|----------|\n",
    "| 1️⃣ | **Configuration** - Set pipeline options | ✗ Required |\n",
    "| 2️⃣ | **Download Data** - Get latest from VM | ✓ Skippable |\n",
    "| 3️⃣ | **Explore Data** - Preview raw data | ✓ Skippable |\n",
    "| 4️⃣ | **Preprocess** - Convert & add features | ✓ Skippable |\n",
    "| 5️⃣ | **Comprehensive EDA** - Deep analysis | ✓ Skippable |\n",
    "| 6️⃣ | **Feature Engineering** - ML features | ✗ Required |\n",
    "| 7️⃣ | **Model Training** - Train & compare | ✗ Required |\n",
    "| 8️⃣ | **Save Results** - Export models | ✓ Skippable |\n",
    "\n",
    "---\n",
    "\n",
    "**Project:** DSP391m Traffic Forecasting - Ho Chi Minh City  \n",
    "**VM:** traffic-forecast-collector (GCP asia-southeast1-a)  \n",
    "**Coverage:** 64 intersections, 144 road segments, 4096m radius\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Quick Start Tips:\n",
    "\n",
    "- **First time?** Run all cells with default config\n",
    "- **Have data already?** Set `USE_EXISTING_DATA = True`\n",
    "- **Quick test?** Disable EDA steps, enable `QUICK_MODE = True`\n",
    "- **Production?** Enable all steps for comprehensive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9379e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1️⃣: Pipeline Configuration\n",
    "\n",
    "**Configure which steps to run** - Customize the pipeline to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 🎛️  PIPELINE CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# ─── Data Source Options ───────────────────────────────────────────\n",
    "USE_EXISTING_DATA = False      # True: Use current data | False: Download new data\n",
    "DOWNLOAD_LATEST_ONLY = True    # True: Latest run only | False: All available runs\n",
    "USE_PREPROCESSED = False       # True: Load from Parquet | False: Process from JSON\n",
    "\n",
    "# ─── Pipeline Steps Control ────────────────────────────────────────\n",
    "ENABLE_DATA_EXPLORATION = True    # Show raw data preview\n",
    "ENABLE_PREPROCESSING = True       # Convert JSON to Parquet\n",
    "ENABLE_COMPREHENSIVE_EDA = True   # Full exploratory analysis (maps, charts)\n",
    "ENABLE_FEATURE_ENGINEERING = True # Create ML features (always needed for training)\n",
    "ENABLE_MODEL_TRAINING = True      # Train and compare models\n",
    "ENABLE_SAVE_MODELS = True         # Save trained models to disk\n",
    "\n",
    "# ─── Analysis Options ──────────────────────────────────────────────\n",
    "QUICK_MODE = False             # True: Faster execution, less detail\n",
    "SHOW_INTERACTIVE_MAPS = True   # Folium geographic visualizations\n",
    "SHOW_PLOTLY_CHARTS = True      # Interactive Plotly charts\n",
    "VERBOSE = True                 # Print detailed progress\n",
    "\n",
    "# ─── Data Paths ────────────────────────────────────────────────────\n",
    "DATA_DIR = '../data/runs'\n",
    "PROCESSED_DIR = '../data/processed'\n",
    "MODELS_DIR = '../traffic_forecast/models/saved'\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Configuration loaded!\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(\"📋 Pipeline Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"🔹 Data Source:\")\n",
    "print(f\"   {'✓' if USE_EXISTING_DATA else '✗'} Use existing data (skip download)\")\n",
    "print(f\"   {'✓' if DOWNLOAD_LATEST_ONLY else '✗'} Download latest only\")\n",
    "print(f\"   {'✓' if USE_PREPROCESSED else '✗'} Use preprocessed Parquet files\")\n",
    "print(f\"\\n🔹 Pipeline Steps:\")\n",
    "print(f\"   {'✓' if ENABLE_DATA_EXPLORATION else '✗'} Data exploration\")\n",
    "print(f\"   {'✓' if ENABLE_PREPROCESSING else '✗'} Preprocessing\")\n",
    "print(f\"   {'✓' if ENABLE_COMPREHENSIVE_EDA else '✗'} Comprehensive EDA\")\n",
    "print(f\"   {'✓' if ENABLE_FEATURE_ENGINEERING else '✗'} Feature engineering\")\n",
    "print(f\"   {'✓' if ENABLE_MODEL_TRAINING else '✗'} Model training\")\n",
    "print(f\"   {'✓' if ENABLE_SAVE_MODELS else '✗'} Save models\")\n",
    "print(f\"\\n🔹 Analysis Options:\")\n",
    "print(f\"   {'✓' if QUICK_MODE else '✗'} Quick mode (faster)\")\n",
    "print(f\"   {'✓' if SHOW_INTERACTIVE_MAPS else '✗'} Interactive maps\")\n",
    "print(f\"   {'✓' if SHOW_PLOTLY_CHARTS else '✗'} Plotly charts\")\n",
    "print(f\"   {'✓' if VERBOSE else '✗'} Verbose output\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Validate configuration\n",
    "if ENABLE_MODEL_TRAINING and not ENABLE_FEATURE_ENGINEERING:\n",
    "    print(\"\\n⚠️  WARNING: Model training requires feature engineering!\")\n",
    "    print(\"   Automatically enabling ENABLE_FEATURE_ENGINEERING\")\n",
    "    ENABLE_FEATURE_ENGINEERING = True\n",
    "\n",
    "if USE_PREPROCESSED and not ENABLE_PREPROCESSING:\n",
    "    print(\"\\n💡 TIP: Using preprocessed data, skipping preprocessing step\")\n",
    "\n",
    "print(\"\\n✨ Ready to run! Execute cells below to start the pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897bdf7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2️⃣: Data Source Selection\n",
    "\n",
    "Choose your data source based on the configuration above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96498536",
   "metadata": {},
   "source": [
    "### Option A: Download Latest Data from VM\n",
    "\n",
    "**Run this cell only if** `USE_EXISTING_DATA = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f221193",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_EXISTING_DATA:\n",
    "    import subprocess\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"🔽 Downloading latest data from VM...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Run download script\n",
    "    result = subprocess.run(\n",
    "        ['bash', 'scripts/data/download_latest.sh'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd='..'  # Run from project root\n",
    "    )\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n✅ Download completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Download failed!\")\n",
    "        print(result.stderr)\n",
    "else:\n",
    "    print(\"⏭️  Skipping download - Using existing data\")\n",
    "    print(\"   Set USE_EXISTING_DATA = False to download new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29d19d",
   "metadata": {},
   "source": [
    "### Option B: Use Existing Data\n",
    "\n",
    "Current data will be loaded from `data/runs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533547ef",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3️⃣: Data Exploration\n",
    "\n",
    "**Preview and validate** the data we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bee095",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DATA_EXPLORATION:\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Find all runs\n",
    "    data_dir = Path(DATA_DIR)\n",
    "    run_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()], reverse=True)\n",
    "    \n",
    "    print(f\"📊 Found {len(run_dirs)} collection runs\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display runs table\n",
    "    runs_info = []\n",
    "    for run_dir in run_dirs[:10]:  # Show latest 10\n",
    "        files = list(run_dir.glob('*.json'))\n",
    "        total_size = sum(f.stat().st_size for f in files)\n",
    "        \n",
    "        # Parse timestamp from run name\n",
    "        run_name = run_dir.name\n",
    "        if run_name.startswith('run_'):\n",
    "            timestamp_str = run_name[4:]  # Remove 'run_' prefix\n",
    "            try:\n",
    "                run_time = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')\n",
    "                time_display = run_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                time_display = timestamp_str\n",
    "        else:\n",
    "            time_display = run_name\n",
    "        \n",
    "        runs_info.append({\n",
    "            'Run': run_name,\n",
    "            'Time': time_display,\n",
    "            'Files': len(files),\n",
    "            'Size (KB)': total_size // 1024\n",
    "        })\n",
    "    \n",
    "    df_runs = pd.DataFrame(runs_info)\n",
    "    print(df_runs.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n💾 Total data: {df_runs['Size (KB)'].sum():,} KB\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping data exploration\")\n",
    "    print(\"   Set ENABLE_DATA_EXPLORATION = True to view data details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebb27f",
   "metadata": {},
   "source": [
    "### 🔍 Inspect Latest Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DATA_EXPLORATION:\n",
    "    # Load latest run\n",
    "    latest_run = run_dirs[0]\n",
    "    print(f\"📁 Latest Run: {latest_run.name}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load all JSON files\n",
    "    files_content = {}\n",
    "    for json_file in latest_run.glob('*.json'):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            files_content[json_file.stem] = json.load(f)\n",
    "        print(f\"✓ Loaded: {json_file.name}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n📊 Data Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'nodes' in files_content:\n",
    "        nodes = files_content['nodes']\n",
    "        print(f\"🔷 Nodes (Intersections): {len(nodes)}\")\n",
    "        if nodes:\n",
    "            print(f\"   Sample: {nodes[0].get('name', 'N/A')}\")\n",
    "            print(f\"   Location: ({nodes[0]['lat']:.6f}, {nodes[0]['lon']:.6f})\")\n",
    "    \n",
    "    if 'edges' in files_content:\n",
    "        edges = files_content['edges']\n",
    "        print(f\"\\n🔶 Edges (Road Segments): {len(edges)}\")\n",
    "        if edges:\n",
    "            print(f\"   Sample: {edges[0]['from_node']} → {edges[0]['to_node']}\")\n",
    "    \n",
    "    if 'traffic_edges' in files_content:\n",
    "        traffic = files_content['traffic_edges']\n",
    "        print(f\"\\n🚦 Traffic Data: {len(traffic)} records\")\n",
    "        if traffic:\n",
    "            sample = traffic[0]\n",
    "            print(f\"   Speed: {sample.get('speed_kmh', 'N/A')} km/h\")\n",
    "            print(f\"   Duration: {sample.get('duration_sec', 'N/A')} sec\")\n",
    "            print(f\"   Distance: {sample.get('distance_km', 'N/A')} km\")\n",
    "            print(f\"   Timestamp: {sample.get('timestamp', 'N/A')}\")\n",
    "    \n",
    "    if 'weather_snapshot' in files_content:\n",
    "        weather = files_content['weather_snapshot']\n",
    "        print(f\"\\n🌤️  Weather Data: {len(weather)} node records\")\n",
    "        if weather:\n",
    "            sample = weather[0]\n",
    "            print(f\"   Temperature: {sample.get('temperature_c', 'N/A')}°C\")\n",
    "            print(f\"   Wind Speed: {sample.get('wind_speed_kmh', 'N/A')} km/h\")\n",
    "            print(f\"   Precipitation: {sample.get('precipitation_mm', 'N/A')} mm\")\n",
    "    \n",
    "    if 'statistics' in files_content:\n",
    "        stats = files_content['statistics']\n",
    "        print(f\"\\n📈 Network Statistics:\")\n",
    "        print(f\"   Total Nodes: {stats.get('total_nodes', 'N/A')}\")\n",
    "        print(f\"   Total Edges: {stats.get('total_edges', 'N/A')}\")\n",
    "        print(f\"   Avg Degree: {stats.get('avg_degree', 'N/A'):.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"⏭️  Skipping detailed exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea7025",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4️⃣: Data Preprocessing\n",
    "\n",
    "**Convert JSON → Parquet** for 10x faster loading + add derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_PREPROCESSING and not USE_PREPROCESSED:\n",
    "    import sys\n",
    "    sys.path.insert(0, '..')  # Add project root to path\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    \n",
    "    print(\"⚙️  Preprocessing data...\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create preprocessor class (inline for notebook)\n",
    "    class SimplePreprocessor:\n",
    "        def __init__(self, data_dir=DATA_DIR, output_dir=PROCESSED_DIR):\n",
    "            self.data_dir = Path(data_dir)\n",
    "            self.output_dir = Path(output_dir)\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        def process_run(self, run_dir):\n",
    "            \"\"\"Process a single run directory\"\"\"\n",
    "            run_name = run_dir.name\n",
    "            if VERBOSE:\n",
    "                print(f\"📦 Processing: {run_name}\")\n",
    "            \n",
    "            # Load JSON files\n",
    "            with open(run_dir / 'traffic_edges.json', 'r') as f:\n",
    "                traffic_data = json.load(f)\n",
    "            \n",
    "            with open(run_dir / 'weather_snapshot.json', 'r') as f:\n",
    "                weather_data = json.load(f)\n",
    "            \n",
    "            with open(run_dir / 'nodes.json', 'r') as f:\n",
    "                nodes_data = json.load(f)\n",
    "            \n",
    "            # Convert to DataFrames\n",
    "            df_traffic = pd.DataFrame(traffic_data)\n",
    "            df_weather = pd.DataFrame(weather_data)\n",
    "            df_nodes = pd.DataFrame(nodes_data)\n",
    "            \n",
    "            # Parse timestamp\n",
    "            df_traffic['timestamp'] = pd.to_datetime(df_traffic['timestamp'])\n",
    "            \n",
    "            # Add time-based features\n",
    "            df_traffic['hour'] = df_traffic['timestamp'].dt.hour\n",
    "            df_traffic['minute'] = df_traffic['timestamp'].dt.minute\n",
    "            df_traffic['day_of_week'] = df_traffic['timestamp'].dt.dayofweek\n",
    "            df_traffic['day_name'] = df_traffic['timestamp'].dt.day_name()\n",
    "            df_traffic['is_weekend'] = df_traffic['day_of_week'].isin([5, 6])\n",
    "            \n",
    "            # Add congestion levels based on speed\n",
    "            def categorize_congestion(speed):\n",
    "                if speed < 15:\n",
    "                    return 'heavy'\n",
    "                elif speed < 25:\n",
    "                    return 'moderate'\n",
    "                elif speed < 35:\n",
    "                    return 'light'\n",
    "                else:\n",
    "                    return 'free_flow'\n",
    "            \n",
    "            df_traffic['congestion_level'] = df_traffic['speed_kmh'].apply(categorize_congestion)\n",
    "            \n",
    "            # Add speed categories\n",
    "            df_traffic['speed_category'] = pd.cut(\n",
    "                df_traffic['speed_kmh'],\n",
    "                bins=[0, 10, 20, 30, 40, 100],\n",
    "                labels=['very_slow', 'slow', 'moderate', 'fast', 'very_fast']\n",
    "            )\n",
    "            \n",
    "            # Merge weather data (average across all nodes)\n",
    "            weather_avg = df_weather.groupby('node_id').first().reset_index()\n",
    "            avg_temp = weather_avg['temperature_c'].mean()\n",
    "            avg_wind = weather_avg['wind_speed_kmh'].mean()\n",
    "            avg_precip = weather_avg['precipitation_mm'].mean()\n",
    "            \n",
    "            df_traffic['temperature_c'] = avg_temp\n",
    "            df_traffic['wind_speed_kmh'] = avg_wind\n",
    "            df_traffic['precipitation_mm'] = avg_precip\n",
    "            \n",
    "            # Add run metadata\n",
    "            df_traffic['run_name'] = run_name\n",
    "            df_traffic['collection_time'] = df_traffic['timestamp'].iloc[0]\n",
    "            \n",
    "            # Save to Parquet\n",
    "            output_file = self.output_dir / f\"{run_name}.parquet\"\n",
    "            df_traffic.to_parquet(output_file, index=False)\n",
    "            if VERBOSE:\n",
    "                print(f\"   ✓ Saved: {output_file.name} ({output_file.stat().st_size // 1024} KB)\")\n",
    "            \n",
    "            return df_traffic\n",
    "        \n",
    "        def process_all(self, limit=None):\n",
    "            \"\"\"Process all runs\"\"\"\n",
    "            run_dirs = sorted([d for d in self.data_dir.iterdir() if d.is_dir()], reverse=True)\n",
    "            \n",
    "            if limit:\n",
    "                run_dirs = run_dirs[:limit]\n",
    "            \n",
    "            all_data = []\n",
    "            for run_dir in run_dirs:\n",
    "                try:\n",
    "                    df = self.process_run(run_dir)\n",
    "                    all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    if VERBOSE:\n",
    "                        print(f\"   ⚠️  Error: {e}\")\n",
    "            \n",
    "            # Combine all runs\n",
    "            if all_data:\n",
    "                df_combined = pd.concat(all_data, ignore_index=True)\n",
    "                combined_file = self.output_dir / 'all_runs_combined.parquet'\n",
    "                df_combined.to_parquet(combined_file, index=False)\n",
    "                print(f\"\\n📊 Combined dataset: {combined_file.name}\")\n",
    "                print(f\"   Records: {len(df_combined):,}\")\n",
    "                print(f\"   Size: {combined_file.stat().st_size // 1024} KB\")\n",
    "                return df_combined\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    # Run preprocessing\n",
    "    preprocessor = SimplePreprocessor()\n",
    "    df_all = preprocessor.process_all()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ Preprocessing completed!\")\n",
    "    print(f\"\\n📈 Total records: {len(df_all):,}\")\n",
    "    print(f\"📅 Date range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "    print(f\"🚦 Avg speed: {df_all['speed_kmh'].mean():.2f} km/h\")\n",
    "\n",
    "elif USE_PREPROCESSED:\n",
    "    # Load from preprocessed Parquet files\n",
    "    print(\"⚡ Loading preprocessed data (fast)...\\n\")\n",
    "    combined_file = Path(PROCESSED_DIR) / 'all_runs_combined.parquet'\n",
    "    if combined_file.exists():\n",
    "        df_all = pd.read_parquet(combined_file)\n",
    "        print(f\"✅ Loaded {len(df_all):,} records from {combined_file.name}\")\n",
    "        print(f\"📅 Date range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "        print(f\"🚦 Avg speed: {df_all['speed_kmh'].mean():.2f} km/h\")\n",
    "    else:\n",
    "        print(f\"❌ Preprocessed file not found: {combined_file}\")\n",
    "        print(\"   Set ENABLE_PREPROCESSING = True to create it\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping preprocessing\")\n",
    "    print(\"   Set ENABLE_PREPROCESSING = True to process data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac48dd9",
   "metadata": {},
   "source": [
    "### 🔍 Preview Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d14767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 Dataset Info:\\n\")\n",
    "print(df_all.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n📊 First 5 rows:\\n\")\n",
    "print(df_all.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n📈 Statistical Summary:\\n\")\n",
    "print(df_all[['speed_kmh', 'duration_sec', 'distance_km', 'temperature_c', 'wind_speed_kmh']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff5c28",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5️⃣: Comprehensive Exploratory Data Analysis\n",
    "\n",
    "**Deep dive into patterns** - Geographic maps, correlations, time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_COMPREHENSIVE_EDA:\n",
    "    print(\"📊 Running Comprehensive EDA...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Import visualization libraries\n",
    "    if SHOW_PLOTLY_CHARTS:\n",
    "        import plotly.express as px\n",
    "        import plotly.graph_objects as go\n",
    "        from plotly.subplots import make_subplots\n",
    "    \n",
    "    if SHOW_INTERACTIVE_MAPS:\n",
    "        import folium\n",
    "        from folium.plugins import HeatMap\n",
    "    \n",
    "    print(\"✅ Visualization libraries loaded\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping comprehensive EDA\")\n",
    "    print(\"   Set ENABLE_COMPREHENSIVE_EDA = True for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02d54e",
   "metadata": {},
   "source": [
    "### 5a. Traffic Speed Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_COMPREHENSIVE_EDA and SHOW_PLOTLY_CHARTS:\n",
    "    # Traffic Speed Analysis\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Speed Distribution', 'Speed Box Plot', \n",
    "                        'Duration Distribution', 'Speed vs Distance'),\n",
    "        specs=[[{'type': 'histogram'}, {'type': 'box'}],\n",
    "               [{'type': 'histogram'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # Speed Histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df_all['speed_kmh'], nbinsx=30, name='Speed',\n",
    "                    marker_color='steelblue', showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Speed Box Plot\n",
    "    fig.add_trace(\n",
    "        go.Box(y=df_all['speed_kmh'], name='Speed',\n",
    "               marker_color='steelblue', showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Duration Histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df_all['duration_sec'], nbinsx=30, name='Duration',\n",
    "                    marker_color='coral', showlegend=False),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Speed vs Distance Scatter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_all['distance_km'], y=df_all['speed_kmh'],\n",
    "                  mode='markers', name='Speed vs Distance',\n",
    "                  marker=dict(color='green', size=6, opacity=0.4),\n",
    "                  showlegend=False),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Speed (km/h)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Speed (km/h)\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Duration (seconds)\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Distance (km)\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Speed (km/h)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Speed (km/h)\", row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(height=700, title_text=\"🚗 Traffic Speed & Duration Analysis\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Print congestion statistics\n",
    "    print(\"\\n📊 Congestion Level Distribution:\")\n",
    "    print(df_all['congestion_level'].value_counts().sort_index())\n",
    "    print(f\"\\n🚦 Average Speed: {df_all['speed_kmh'].mean():.2f} km/h\")\n",
    "    print(f\"⏱️  Average Duration: {df_all['duration_sec'].mean():.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cf74e",
   "metadata": {},
   "source": [
    "### 5b. Hourly Traffic Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_COMPREHENSIVE_EDA:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Hourly patterns\n",
    "    hourly_speed = df_all.groupby('hour')['speed_kmh'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    fig.suptitle('📈 Hourly Traffic Patterns', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Average speed by hour\n",
    "    axes[0].plot(hourly_speed['hour'], hourly_speed['mean'], marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "    axes[0].fill_between(\n",
    "        hourly_speed['hour'],\n",
    "        hourly_speed['mean'] - hourly_speed['std'],\n",
    "        hourly_speed['mean'] + hourly_speed['std'],\n",
    "        alpha=0.3,\n",
    "        color='steelblue'\n",
    "    )\n",
    "    axes[0].set_xlabel('Hour of Day', fontsize=12)\n",
    "    axes[0].set_ylabel('Average Speed (km/h)', fontsize=12)\n",
    "    axes[0].set_title('Average Speed by Hour (with std deviation)')\n",
    "    axes[0].set_xticks(range(0, 24, 2))\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=df_all['speed_kmh'].mean(), color='red', linestyle='--', label='Overall Average')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Traffic volume by hour\n",
    "    axes[1].bar(hourly_speed['hour'], hourly_speed['count'], color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Hour of Day', fontsize=12)\n",
    "    axes[1].set_ylabel('Number of Records', fontsize=12)\n",
    "    axes[1].set_title('Traffic Data Volume by Hour')\n",
    "    axes[1].set_xticks(range(0, 24, 2))\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Peak hours identification\n",
    "    peak_hours = hourly_speed.nsmallest(3, 'mean')[['hour', 'mean']]\n",
    "    print(\"\\n🔴 Slowest Hours (Peak Congestion):\")\n",
    "    for _, row in peak_hours.iterrows():\n",
    "        print(f\"   • Hour {int(row['hour']):02d}:00 - Avg Speed: {row['mean']:.2f} km/h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cddffb",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6️⃣: Feature Engineering for ML\n",
    "\n",
    "**Create advanced features** for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99328b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_FEATURE_ENGINEERING:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    print(\"🔧 Engineering features...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Sort by timestamp for lag features\n",
    "    df_all = df_all.sort_values(['edge_id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    # 1. Lag features (previous speeds)\n",
    "    print(\"1️⃣  Creating lag features...\")\n",
    "    for lag in [1, 2, 3]:\n",
    "        df_all[f'speed_lag_{lag}'] = df_all.groupby('edge_id')['speed_kmh'].shift(lag)\n",
    "    print(f\"   ✓ Added: speed_lag_1, speed_lag_2, speed_lag_3\")\n",
    "    \n",
    "    # 2. Rolling statistics\n",
    "    print(\"\\n2️⃣  Creating rolling statistics...\")\n",
    "    df_all['speed_rolling_mean_3'] = df_all.groupby('edge_id')['speed_kmh'].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "    df_all['speed_rolling_std_3'] = df_all.groupby('edge_id')['speed_kmh'].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).std()\n",
    "    )\n",
    "    print(f\"   ✓ Added: speed_rolling_mean_3, speed_rolling_std_3\")\n",
    "    \n",
    "    # 3. Cyclical time features\n",
    "    print(\"\\n3️⃣  Creating cyclical time features...\")\n",
    "    df_all['hour_sin'] = np.sin(2 * np.pi * df_all['hour'] / 24)\n",
    "    df_all['hour_cos'] = np.cos(2 * np.pi * df_all['hour'] / 24)\n",
    "    df_all['day_of_week_sin'] = np.sin(2 * np.pi * df_all['day_of_week'] / 7)\n",
    "    df_all['day_of_week_cos'] = np.cos(2 * np.pi * df_all['day_of_week'] / 7)\n",
    "    print(f\"   ✓ Added: hour_sin, hour_cos, day_of_week_sin, day_of_week_cos\")\n",
    "    \n",
    "    # 4. Rush hour indicators\n",
    "    print(\"\\n4️⃣  Creating rush hour indicators...\")\n",
    "    df_all['is_morning_rush'] = df_all['hour'].isin([7, 8, 9]).astype(int)\n",
    "    df_all['is_evening_rush'] = df_all['hour'].isin([17, 18, 19]).astype(int)\n",
    "    df_all['is_rush_hour'] = (df_all['is_morning_rush'] | df_all['is_evening_rush']).astype(int)\n",
    "    print(f\"   ✓ Added: is_morning_rush, is_evening_rush, is_rush_hour\")\n",
    "    \n",
    "    # 5. Encode categorical features\n",
    "    print(\"\\n5️⃣  Encoding categorical features...\")\n",
    "    le_congestion = LabelEncoder()\n",
    "    df_all['congestion_level_encoded'] = le_congestion.fit_transform(df_all['congestion_level'])\n",
    "    print(f\"   ✓ Added: congestion_level_encoded\")\n",
    "    \n",
    "    # Drop rows with NaN (from lag features)\n",
    "    df_features = df_all.dropna().copy()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"✅ Feature engineering completed!\")\n",
    "    print(f\"\\n📊 Dataset shape: {df_features.shape}\")\n",
    "    print(f\"📋 Total features: {df_features.shape[1]}\")\n",
    "    print(f\"🚫 Dropped {len(df_all) - len(df_features)} rows with missing lag values\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping feature engineering\")\n",
    "    print(\"   ⚠️  WARNING: Required for model training!\")\n",
    "    df_features = df_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575fb1b",
   "metadata": {},
   "source": [
    "### 📋 Feature List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd317c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📝 All Features:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "feature_groups = {\n",
    "    '🚦 Traffic Features': ['speed_kmh', 'duration_sec', 'distance_km', 'congestion_level', 'speed_category'],\n",
    "    '⏱️  Time Features': ['hour', 'minute', 'day_of_week', 'day_name', 'is_weekend', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos'],\n",
    "    '🌤️  Weather Features': ['temperature_c', 'wind_speed_kmh', 'precipitation_mm'],\n",
    "    '📊 Lag Features': ['speed_lag_1', 'speed_lag_2', 'speed_lag_3'],\n",
    "    '📈 Rolling Features': ['speed_rolling_mean_3', 'speed_rolling_std_3'],\n",
    "    '🚨 Rush Hour Features': ['is_morning_rush', 'is_evening_rush', 'is_rush_hour'],\n",
    "    '🔢 Encoded Features': ['congestion_level_encoded']\n",
    "}\n",
    "\n",
    "for group_name, features in feature_groups.items():\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    for feat in features:\n",
    "        if feat in df_features.columns:\n",
    "            print(f\"   ✓ {feat}\")\n",
    "        else:\n",
    "            print(f\"   ✗ {feat} (missing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42517a9a",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7️⃣: Train & Compare Models\n",
    "\n",
    "**Train multiple models** and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    print(\"✂️  Splitting data...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Define features for modeling\n",
    "    feature_columns = [\n",
    "        # Time features\n",
    "        'hour', 'day_of_week', 'is_weekend', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "        # Traffic features\n",
    "        'distance_km', 'duration_sec',\n",
    "        # Weather features\n",
    "        'temperature_c', 'wind_speed_kmh', 'precipitation_mm',\n",
    "        # Lag features\n",
    "        'speed_lag_1', 'speed_lag_2', 'speed_lag_3',\n",
    "        # Rolling features\n",
    "        'speed_rolling_mean_3', 'speed_rolling_std_3',\n",
    "        # Rush hour\n",
    "        'is_morning_rush', 'is_evening_rush', 'is_rush_hour'\n",
    "    ]\n",
    "    \n",
    "    target_column = 'speed_kmh'\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df_features[feature_columns].copy()\n",
    "    y = df_features[target_column].copy()\n",
    "    \n",
    "    # Time-based split (last 20% as test)\n",
    "    split_idx = int(len(df_features) * 0.8)\n",
    "    X_train = X.iloc[:split_idx]\n",
    "    X_test = X.iloc[split_idx:]\n",
    "    y_train = y.iloc[:split_idx]\n",
    "    y_test = y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"📊 Dataset split (time-based):\") \n",
    "    print(f\"   Training set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    print(f\"\\n📋 Features: {len(feature_columns)}\")\n",
    "    print(f\"🎯 Target: {target_column}\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping train-test split\")\n",
    "    print(\"   Set ENABLE_MODEL_TRAINING = True to train models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e13c3a7",
   "metadata": {},
   "source": [
    "### 📊 Model Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING:\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import LinearRegression, Ridge\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    import time\n",
    "    \n",
    "    print(\"🤖 Training models...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Define models\n",
    "    if QUICK_MODE:\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1),\n",
    "        }\n",
    "        print(\"⚡ Quick Mode: Training 2 fast models\\n\")\n",
    "    else:\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge Regression': Ridge(alpha=1.0),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "        }\n",
    "        print(\"🔬 Full Mode: Training 4 models\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"🔧 Training: {model_name}\")\n",
    "        \n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Train Time (s)': train_time\n",
    "        })\n",
    "        \n",
    "        print(f\"   ✓ MAE: {mae:.3f} km/h\")\n",
    "        print(f\"   ✓ RMSE: {rmse:.3f} km/h\")\n",
    "        print(f\"   ✓ R²: {r2:.3f}\")\n",
    "        print(f\"   ⏱️  Time: {train_time:.2f}s\\n\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"✅ All models trained!\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping model training\")\n",
    "    print(\"   Set ENABLE_MODEL_TRAINING = True to train models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d750f6",
   "metadata": {},
   "source": [
    "### 📊 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING:\n",
    "    # Create comparison table\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results.sort_values('RMSE')\n",
    "    \n",
    "    print(\"\\n🏆 Model Performance Comparison:\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    best_model_name = df_results.iloc[0]['Model']\n",
    "    best_rmse = df_results.iloc[0]['RMSE']\n",
    "    best_r2 = df_results.iloc[0]['R²']\n",
    "    \n",
    "    print(f\"\\n🥇 Best Model: {best_model_name}\")\n",
    "    print(f\"   RMSE: {best_rmse:.3f} km/h\")\n",
    "    print(f\"   R²: {best_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1655fb",
   "metadata": {},
   "source": [
    "### 📈 Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be646728",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('Model Performance Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # MAE comparison\n",
    "    axes[0].barh(df_results['Model'], df_results['MAE'], color='skyblue')\n",
    "    axes[0].set_xlabel('MAE (km/h) - Lower is Better')\n",
    "    axes[0].set_title('Mean Absolute Error')\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # RMSE comparison\n",
    "    axes[1].barh(df_results['Model'], df_results['RMSE'], color='lightcoral')\n",
    "    axes[1].set_xlabel('RMSE (km/h) - Lower is Better')\n",
    "    axes[1].set_title('Root Mean Squared Error')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # R² comparison\n",
    "    axes[2].barh(df_results['Model'], df_results['R²'], color='lightgreen')\n",
    "    axes[2].set_xlabel('R² Score - Higher is Better')\n",
    "    axes[2].set_title('R² Score')\n",
    "    axes[2].set_xlim(0, 1)\n",
    "    axes[2].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5b168",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8️⃣: Save Models & Results\n",
    "\n",
    "**Export trained models** for deployment and future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9066e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_SAVE_MODELS and ENABLE_MODEL_TRAINING:\n",
    "    import joblib\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Create models directory\n",
    "    models_dir = Path(MODELS_DIR)\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"💾 Saving models...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Save all models\n",
    "    for model_name, model in models.items():\n",
    "        filename = model_name.lower().replace(' ', '_') + '.pkl'\n",
    "        filepath = models_dir / filename\n",
    "        joblib.dump(model, filepath)\n",
    "        print(f\"✓ Saved: {filename} ({filepath.stat().st_size // 1024} KB)\")\n",
    "    \n",
    "    # Save feature columns\n",
    "    feature_info = {\n",
    "        'feature_columns': feature_columns,\n",
    "        'target_column': target_column,\n",
    "        'num_features': len(feature_columns)\n",
    "    }\n",
    "    joblib.dump(feature_info, models_dir / 'feature_info.pkl')\n",
    "    print(f\"\\n✓ Saved feature info\")\n",
    "    \n",
    "    # Save results\n",
    "    df_results.to_csv(models_dir / 'model_comparison.csv', index=False)\n",
    "    print(f\"✓ Saved model comparison\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"✅ All artifacts saved to: {models_dir}\")\n",
    "elif not ENABLE_MODEL_TRAINING:\n",
    "    print(\"⏭️  No models to save (training was skipped)\")\n",
    "else:\n",
    "    print(\"⏭️  Skipping model saving\")\n",
    "    print(\"   Set ENABLE_SAVE_MODELS = True to save models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289eccad",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎉 Pipeline Complete!\n",
    "\n",
    "**Summary of execution** and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5374530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"╔\" + \"═\" * 68 + \"╗\")\n",
    "print(\"║\" + \" \" * 20 + \"🎉 PIPELINE EXECUTION COMPLETE!\" + \" \" * 17 + \"║\")\n",
    "print(\"╚\" + \"═\" * 68 + \"╝\")\n",
    "print()\n",
    "\n",
    "# Summary\n",
    "print(\"📊 EXECUTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "steps_summary = [\n",
    "    (\"Configuration\", True, \"✅ Loaded\"),\n",
    "    (\"Download Data\", not USE_EXISTING_DATA, \"✅ Downloaded\" if not USE_EXISTING_DATA else \"⏭️  Skipped (used existing)\"),\n",
    "    (\"Data Exploration\", ENABLE_DATA_EXPLORATION, \"✅ Explored\" if ENABLE_DATA_EXPLORATION else \"⏭️  Skipped\"),\n",
    "    (\"Preprocessing\", ENABLE_PREPROCESSING, \"✅ Processed\" if ENABLE_PREPROCESSING else \"⏭️  Skipped\"),\n",
    "    (\"Comprehensive EDA\", ENABLE_COMPREHENSIVE_EDA, \"✅ Analyzed\" if ENABLE_COMPREHENSIVE_EDA else \"⏭️  Skipped\"),\n",
    "    (\"Feature Engineering\", ENABLE_FEATURE_ENGINEERING, f\"✅ Created {len(feature_columns)} features\" if ENABLE_FEATURE_ENGINEERING else \"⏭️  Skipped\"),\n",
    "    (\"Model Training\", ENABLE_MODEL_TRAINING, f\"✅ Trained {len(models)} models\" if ENABLE_MODEL_TRAINING else \"⏭️  Skipped\"),\n",
    "    (\"Save Models\", ENABLE_SAVE_MODELS and ENABLE_MODEL_TRAINING, \"✅ Saved\" if ENABLE_SAVE_MODELS and ENABLE_MODEL_TRAINING else \"⏭️  Skipped\"),\n",
    "]\n",
    "\n",
    "for step_name, executed, status in steps_summary:\n",
    "    print(f\"• {step_name:.<25} {status}\")\n",
    "\n",
    "if ENABLE_MODEL_TRAINING:\n",
    "    print(\"\\n📈 BEST MODEL RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"• Model: {best_model_name}\")\n",
    "    print(f\"• RMSE: {best_rmse:.3f} km/h\")\n",
    "    print(f\"• MAE: {df_results.iloc[0]['MAE']:.3f} km/h\")\n",
    "    print(f\"• R² Score: {best_r2:.3f}\")\n",
    "    print(f\"• Training Time: {df_results.iloc[0]['Train Time (s)']:.2f}s\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. 🔧 Hyperparameter Tuning - Optimize best model with GridSearch\")\n",
    "print(\"2. 🧠 Advanced Models - Try LSTM, GNN for temporal/spatial patterns\")\n",
    "print(\"3. 🎯 Ensemble Methods - Combine multiple models\")\n",
    "print(\"4. 🌐 Deploy API - Create endpoint for real-time predictions\")\n",
    "print(\"5. 📊 Dashboard - Build real-time monitoring dashboard\")\n",
    "\n",
    "print(\"\\n💡 TO RE-RUN THIS PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"• Just 'Run All Cells' again!\")\n",
    "print(\"• Or adjust configuration in Step 1 and re-run\")\n",
    "print(\"• Set USE_EXISTING_DATA = False to download fresh data\")\n",
    "print(\"• Enable/disable steps as needed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✨ Thank you for using the Traffic Forecasting Pipeline!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
