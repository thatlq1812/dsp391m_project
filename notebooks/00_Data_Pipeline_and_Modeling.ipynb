{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc089555",
   "metadata": {},
   "source": [
    "# Traffic Forecasting with Deep Learning - HCMC\n",
    "\n",
    "**Complete Pipeline**: Data Loading -> Preprocessing -> Feature Engineering -> DL Model Training\n",
    "\n",
    "**Author:** thatlq1812  \n",
    "**Project:** DSP391m Traffic Forecasting System  \n",
    "**Models:** LSTM and ATSCGN (Deep Learning)\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "| Step | Description | Required |\n",
    "|------|-------------|----------|\n",
    "| 1 | Configuration - Set pipeline options | Yes |\n",
    "| 2 | Data Loading - Load collected traffic data | Yes |\n",
    "| 3 | Data Exploration - Preview and understand data | Optional |\n",
    "| 4 | Preprocessing - Clean and prepare data | Yes |\n",
    "| 5 | Feature Engineering - Create features for DL models | Yes |\n",
    "| 6 | Model Training - Train LSTM and ATSCGN | Yes |\n",
    "| 7 | Evaluation - Compare model performance | Yes |\n",
    "| 8 | Save Models - Export for production | Optional |\n",
    "\n",
    "---\n",
    "\n",
    "**Coverage:** 78 nodes, 234 road segments, 4096m radius  \n",
    "**Data Source:** Google Directions API + Open-Meteo Weather  \n",
    "**VM:** traffic-forecast-collector (GCP asia-southeast1-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f09cb9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Pipeline Configuration\n",
    "\n",
    "Configure which steps to run and analysis options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3693067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "\n",
      "Pipeline Configuration:\n",
      "============================================================\n",
      "Data Exploration:        Enabled\n",
      "Preprocessing:           Enabled\n",
      "Feature Engineering:     Enabled\n",
      "Model Training:          Enabled\n",
      "  - LSTM:                Yes\n",
      "  - ATSCGN:              Yes\n",
      "Evaluation:              Enabled\n",
      "Save Models:             Enabled\n",
      "\n",
      "Training Parameters:\n",
      "============================================================\n",
      "Epochs:                  50\n",
      "Batch Size:              32\n",
      "Learning Rate:           0.001\n",
      "Validation Split:        0.2\n",
      "Sequence Length:         12\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Pipeline Configuration\n",
    "# ======================\n",
    "\n",
    "# Data Source\n",
    "DATA_DIR = '../data/runs'\n",
    "PROCESSED_DIR = '../data/processed'\n",
    "MODELS_DIR = '../models'\n",
    "\n",
    "# Pipeline Steps Control\n",
    "ENABLE_DATA_EXPLORATION = True      # Show data preview and statistics\n",
    "ENABLE_PREPROCESSING = True         # Clean and prepare data\n",
    "ENABLE_FEATURE_ENGINEERING = True   # Create features for models\n",
    "ENABLE_MODEL_TRAINING = True        # Train LSTM and ATSCGN\n",
    "ENABLE_EVALUATION = True            # Evaluate and compare models\n",
    "ENABLE_SAVE_MODELS = True           # Save trained models\n",
    "\n",
    "# Model Configuration\n",
    "TRAIN_LSTM = True                   # Train LSTM model\n",
    "TRAIN_ATSCGN = True                 # Train ATSCGN model\n",
    "\n",
    "# Training Parameters\n",
    "EPOCHS = 50                         # Number of training epochs\n",
    "BATCH_SIZE = 32                     # Batch size for training\n",
    "LEARNING_RATE = 0.001               # Learning rate\n",
    "VALIDATION_SPLIT = 0.2              # Validation data percentage\n",
    "SEQUENCE_LENGTH = 12                # Input sequence length (12 = 3 hours at 15-min intervals)\n",
    "\n",
    "# Analysis Options\n",
    "VERBOSE = True                      # Print detailed progress\n",
    "PLOT_TRAINING_HISTORY = True        # Plot training curves\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(\"\\nPipeline Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Data Exploration:        {'Enabled' if ENABLE_DATA_EXPLORATION else 'Disabled'}\")\n",
    "print(f\"Preprocessing:           {'Enabled' if ENABLE_PREPROCESSING else 'Disabled'}\")\n",
    "print(f\"Feature Engineering:     {'Enabled' if ENABLE_FEATURE_ENGINEERING else 'Disabled'}\")\n",
    "print(f\"Model Training:          {'Enabled' if ENABLE_MODEL_TRAINING else 'Disabled'}\")\n",
    "print(f\"  - LSTM:                {'Yes' if TRAIN_LSTM else 'No'}\")\n",
    "print(f\"  - ATSCGN:              {'Yes' if TRAIN_ATSCGN else 'No'}\")\n",
    "print(f\"Evaluation:              {'Enabled' if ENABLE_EVALUATION else 'Disabled'}\")\n",
    "print(f\"Save Models:             {'Enabled' if ENABLE_SAVE_MODELS else 'Disabled'}\")\n",
    "print(\"\\nTraining Parameters:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Epochs:                  {EPOCHS}\")\n",
    "print(f\"Batch Size:              {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate:           {LEARNING_RATE}\")\n",
    "print(f\"Validation Split:        {VALIDATION_SPLIT}\")\n",
    "print(f\"Sequence Length:         {SEQUENCE_LENGTH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa75a45",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2116f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.1\n",
      "\n",
      "Libraries imported successfully!\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    HAS_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed. Please install: pip install tensorflow\")\n",
    "    HAS_TENSORFLOW = False\n",
    "\n",
    "# Project Modules\n",
    "sys.path.append('..')\n",
    "from traffic_forecast.ml import DataLoader, DataPreprocessor\n",
    "from traffic_forecast.models import registry\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\nLibraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501ed96",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Load Data\n",
    "\n",
    "Load collected traffic data from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca78499c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data using DataLoader\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading traffic data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "File \u001b[1;32md:\\UNI\\DSP391m\\project\\notebooks\\..\\traffic_forecast\\ml\\data_loader.py:29\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, data_dir)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;129;01mor\u001b[39;00m PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownloads\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scan_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\UNI\\DSP391m\\project\\notebooks\\..\\traffic_forecast\\ml\\data_loader.py:33\u001b[0m, in \u001b[0;36mDataLoader._scan_runs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_scan_runs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan data directory for available runs.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m():\n\u001b[0;32m     34\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "# Load data using DataLoader\n",
    "loader = DataLoader(data_dir=DATA_DIR)\n",
    "\n",
    "print(\"Loading traffic data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get available runs\n",
    "available_runs = loader.list_available_runs()\n",
    "print(f\"Found {len(available_runs)} collection runs\")\n",
    "\n",
    "if len(available_runs) == 0:\n",
    "    print(\"\\nNo data found! Please collect data first:\")\n",
    "    print(\"  python scripts/collect_once.py\")\n",
    "    raise ValueError(\"No data available\")\n",
    "\n",
    "# Load all available data\n",
    "data = loader.load_all_runs()\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"Total records: {len(data)}\")\n",
    "print(f\"Date range: {data['timestamp'].min()} to {data['timestamp'].max()}\")\n",
    "print(f\"\\nColumns: {list(data.columns)}\")\n",
    "print(\"\\nFirst few records:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316d145",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Data Exploration (Optional)\n",
    "\n",
    "Explore and understand the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2770193",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DATA_EXPLORATION:\n",
    "    print(\"Data Exploration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n1. Dataset Info:\")\n",
    "    print(f\"   Shape: {data.shape}\")\n",
    "    print(f\"   Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n2. Data Types:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    print(\"\\n3. Missing Values:\")\n",
    "    missing = data.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"   No missing values\")\n",
    "    \n",
    "    print(\"\\n4. Statistical Summary:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    # Visualizations\n",
    "    if 'speed_kmh' in data.columns:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Speed distribution\n",
    "        axes[0, 0].hist(data['speed_kmh'], bins=50, edgecolor='black')\n",
    "        axes[0, 0].set_title('Speed Distribution')\n",
    "        axes[0, 0].set_xlabel('Speed (km/h)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Speed over time\n",
    "        data_sample = data.sample(min(1000, len(data))).sort_values('timestamp')\n",
    "        axes[0, 1].scatter(data_sample['timestamp'], data_sample['speed_kmh'], alpha=0.5)\n",
    "        axes[0, 1].set_title('Speed Over Time (Sample)')\n",
    "        axes[0, 1].set_xlabel('Timestamp')\n",
    "        axes[0, 1].set_ylabel('Speed (km/h)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Speed by hour (if hour column exists)\n",
    "        if 'hour' in data.columns:\n",
    "            hourly_speed = data.groupby('hour')['speed_kmh'].mean()\n",
    "            axes[1, 0].plot(hourly_speed.index, hourly_speed.values, marker='o')\n",
    "            axes[1, 0].set_title('Average Speed by Hour')\n",
    "            axes[1, 0].set_xlabel('Hour of Day')\n",
    "            axes[1, 0].set_ylabel('Average Speed (km/h)')\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # Speed boxplot by day of week (if available)\n",
    "        if 'day_of_week' in data.columns:\n",
    "            data.boxplot(column='speed_kmh', by='day_of_week', ax=axes[1, 1])\n",
    "            axes[1, 1].set_title('Speed Distribution by Day of Week')\n",
    "            axes[1, 1].set_xlabel('Day of Week')\n",
    "            axes[1, 1].set_ylabel('Speed (km/h)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nData exploration complete!\")\n",
    "else:\n",
    "    print(\"Data exploration skipped (ENABLE_DATA_EXPLORATION = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539973a1",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Data Preprocessing\n",
    "\n",
    "Clean and prepare data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_PREPROCESSING:\n",
    "    print(\"Data Preprocessing\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = DataPreprocessor()\n",
    "    \n",
    "    # Preprocess data\n",
    "    print(\"\\nCleaning and transforming data...\")\n",
    "    data_processed = preprocessor.fit_transform(data)\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Records before: {len(data)}\")\n",
    "    print(f\"Records after: {len(data_processed)}\")\n",
    "    print(f\"Columns: {list(data_processed.columns)}\")\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "    output_file = os.path.join(PROCESSED_DIR, 'traffic_data_processed.parquet')\n",
    "    data_processed.to_parquet(output_file, index=False)\n",
    "    print(f\"\\nSaved preprocessed data to: {output_file}\")\n",
    "    \n",
    "    # Use processed data going forward\n",
    "    data = data_processed\n",
    "else:\n",
    "    print(\"Preprocessing skipped (ENABLE_PREPROCESSING = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd136f8",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Feature Engineering\n",
    "\n",
    "Create features for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_FEATURE_ENGINEERING:\n",
    "    print(\"Feature Engineering for Deep Learning\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    from traffic_forecast.ml.features import (\n",
    "        add_temporal_features,\n",
    "        add_spatial_features,\n",
    "        add_weather_features,\n",
    "        add_traffic_features\n",
    "    )\n",
    "    \n",
    "    # Add temporal features\n",
    "    print(\"\\n1. Adding temporal features...\")\n",
    "    data = add_temporal_features(data)\n",
    "    print(f\"   Added: hour, day_of_week, is_weekend, is_peak_hour\")\n",
    "    \n",
    "    # Add spatial features (if node coordinates available)\n",
    "    print(\"\\n2. Adding spatial features...\")\n",
    "    try:\n",
    "        data = add_spatial_features(data)\n",
    "        print(f\"   Added: distance, bearing, etc.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Skipped: {e}\")\n",
    "    \n",
    "    # Add weather features (if available)\n",
    "    print(\"\\n3. Adding weather features...\")\n",
    "    try:\n",
    "        data = add_weather_features(data)\n",
    "        print(f\"   Added: temperature, precipitation, etc.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Skipped: {e}\")\n",
    "    \n",
    "    # Add traffic features (lag features)\n",
    "    print(\"\\n4. Adding traffic lag features...\")\n",
    "    try:\n",
    "        data = add_traffic_features(data)\n",
    "        print(f\"   Added: speed lags, rolling averages\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Skipped: {e}\")\n",
    "    \n",
    "    print(f\"\\nFeature engineering complete!\")\n",
    "    print(f\"Total features: {len(data.columns)}\")\n",
    "    print(f\"Feature names: {list(data.columns)}\")\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    data = data.dropna()\n",
    "    print(f\"\\nRecords after dropping NaN: {len(data)}\")\n",
    "else:\n",
    "    print(\"Feature engineering skipped (ENABLE_FEATURE_ENGINEERING = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeae258",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Prepare Training Data\n",
    "\n",
    "Create sequences for time-series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length, target_col='speed_kmh'):\n",
    "    \"\"\"Create sequences for time-series prediction.\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    data_sorted = data.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Select feature columns (exclude timestamp and target)\n",
    "    feature_cols = [col for col in data_sorted.columns \n",
    "                    if col not in ['timestamp', target_col]]\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(data_sorted) - sequence_length):\n",
    "        X.append(data_sorted[feature_cols].iloc[i:i+sequence_length].values)\n",
    "        y.append(data_sorted[target_col].iloc[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y), feature_cols\n",
    "\n",
    "print(\"Preparing training sequences...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create sequences\n",
    "X, y, feature_names = create_sequences(data, SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"\\nSequences created!\")\n",
    "print(f\"X shape: {X.shape} (samples, timesteps, features)\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain-Test Split:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09b0b8",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Train LSTM Model\n",
    "\n",
    "Train LSTM model for traffic speed prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING and TRAIN_LSTM and HAS_TENSORFLOW:\n",
    "    print(\"Training LSTM Model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Build LSTM model\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    \n",
    "    model_lstm = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, X_train.shape[2])),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_lstm.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model_lstm.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    os.makedirs(os.path.join(MODELS_DIR, 'checkpoints'), exist_ok=True)\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ModelCheckpoint(\n",
    "            os.path.join(MODELS_DIR, 'checkpoints', 'lstm_best.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining LSTM model...\")\n",
    "    history_lstm = model_lstm.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1 if VERBOSE else 0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating LSTM model...\")\n",
    "    test_loss, test_mae = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f} km/h\")\n",
    "    \n",
    "    # Plot training history\n",
    "    if PLOT_TRAINING_HISTORY:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Loss\n",
    "        axes[0].plot(history_lstm.history['loss'], label='Training Loss')\n",
    "        axes[0].plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
    "        axes[0].set_title('LSTM Model Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss (MSE)')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # MAE\n",
    "        axes[1].plot(history_lstm.history['mae'], label='Training MAE')\n",
    "        axes[1].plot(history_lstm.history['val_mae'], label='Validation MAE')\n",
    "        axes[1].set_title('LSTM Model MAE')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('MAE (km/h)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nLSTM training complete!\")\n",
    "else:\n",
    "    if not HAS_TENSORFLOW:\n",
    "        print(\"LSTM training skipped (TensorFlow not available)\")\n",
    "    else:\n",
    "        print(\"LSTM training skipped (TRAIN_LSTM = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0dd3a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Train ATSCGN Model\n",
    "\n",
    "Train ATSCGN (Adaptive Traffic Spatial-Temporal Convolutional Graph Network) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING and TRAIN_ATSCGN and HAS_TENSORFLOW:\n",
    "    print(\"Training ATSCGN Model\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nNote: ATSCGN requires graph adjacency matrix.\")\n",
    "    print(\"This is a placeholder for future implementation.\")\n",
    "    print(\"\\nFor now, use LSTM for traffic forecasting.\")\n",
    "    print(\"ATSCGN implementation coming soon!\")\n",
    "    \n",
    "    # TODO: Implement ATSCGN training\n",
    "    # from traffic_forecast.models.graph import ASTGCNTrafficModel\n",
    "    # model_atscgn = ASTGCNTrafficModel(...)\n",
    "    \n",
    "else:\n",
    "    if not HAS_TENSORFLOW:\n",
    "        print(\"ATSCGN training skipped (TensorFlow not available)\")\n",
    "    else:\n",
    "        print(\"ATSCGN training skipped (TRAIN_ATSCGN = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd4773",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Model Evaluation and Comparison\n",
    "\n",
    "Evaluate and compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_EVALUATION and TRAIN_LSTM and HAS_TENSORFLOW:\n",
    "    print(\"Model Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_lstm = model_lstm.predict(X_test).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred_lstm)\n",
    "    mae = mean_absolute_error(y_test, y_pred_lstm)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred_lstm)\n",
    "    \n",
    "    print(\"\\nLSTM Model Performance:\")\n",
    "    print(f\"  MSE:  {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f} km/h\")\n",
    "    print(f\"  MAE:  {mae:.4f} km/h\")\n",
    "    print(f\"  R2:   {r2:.4f}\")\n",
    "    \n",
    "    # Visualize predictions\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Predictions vs Actual (sample)\n",
    "    sample_size = min(200, len(y_test))\n",
    "    axes[0].plot(y_test[:sample_size], label='Actual', alpha=0.7)\n",
    "    axes[0].plot(y_pred_lstm[:sample_size], label='Predicted', alpha=0.7)\n",
    "    axes[0].set_title('LSTM Predictions vs Actual (Sample)')\n",
    "    axes[0].set_xlabel('Sample Index')\n",
    "    axes[0].set_ylabel('Speed (km/h)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[1].scatter(y_test, y_pred_lstm, alpha=0.5)\n",
    "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', label='Perfect Prediction')\n",
    "    axes[1].set_title('LSTM Predictions vs Actual (Scatter)')\n",
    "    axes[1].set_xlabel('Actual Speed (km/h)')\n",
    "    axes[1].set_ylabel('Predicted Speed (km/h)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Error distribution\n",
    "    errors = y_test - y_pred_lstm\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=50, edgecolor='black')\n",
    "    plt.axvline(x=0, color='r', linestyle='--', label='Zero Error')\n",
    "    plt.title('LSTM Prediction Error Distribution')\n",
    "    plt.xlabel('Error (km/h)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nEvaluation complete!\")\n",
    "else:\n",
    "    print(\"Evaluation skipped (ENABLE_EVALUATION = False or no trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d12c9d",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Save Models\n",
    "\n",
    "Save trained models for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82951b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_SAVE_MODELS and TRAIN_LSTM and HAS_TENSORFLOW:\n",
    "    print(\"Saving Models\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create models directory\n",
    "    os.makedirs(os.path.join(MODELS_DIR, 'production'), exist_ok=True)\n",
    "    \n",
    "    # Save LSTM model\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    lstm_path = os.path.join(MODELS_DIR, 'production', f'lstm_model_{timestamp}.h5')\n",
    "    model_lstm.save(lstm_path)\n",
    "    print(f\"\\nLSTM model saved to: {lstm_path}\")\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'model_type': 'LSTM',\n",
    "        'timestamp': timestamp,\n",
    "        'sequence_length': SEQUENCE_LENGTH,\n",
    "        'num_features': X_train.shape[2],\n",
    "        'feature_names': feature_names,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'test_mse': float(mse),\n",
    "        'test_mae': float(mae),\n",
    "        'test_rmse': float(rmse),\n",
    "        'test_r2': float(r2)\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(MODELS_DIR, 'production', f'lstm_metadata_{timestamp}.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Model metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    print(\"\\nModels saved successfully!\")\n",
    "else:\n",
    "    print(\"Model saving skipped (ENABLE_SAVE_MODELS = False or no trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d770df25",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Pipeline execution complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Total records: {len(data)}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "\n",
    "if TRAIN_LSTM and HAS_TENSORFLOW:\n",
    "    print(f\"\\nLSTM Model:\")\n",
    "    print(f\"  Test RMSE: {rmse:.4f} km/h\")\n",
    "    print(f\"  Test MAE: {mae:.4f} km/h\")\n",
    "    print(f\"  Test R2: {r2:.4f}\")\n",
    "    if ENABLE_SAVE_MODELS:\n",
    "        print(f\"  Model saved: {lstm_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"  1. Use saved model for predictions\")\n",
    "print(\"  2. Deploy to production (GCP VM)\")\n",
    "print(\"  3. Monitor model performance\")\n",
    "print(\"  4. Retrain with more data as needed\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d392e4b",
   "metadata": {},
   "source": [
    "# Complete Traffic Forecasting Pipeline - HCMC\n",
    "\n",
    "**All-in-One Workflow**: Download → Preprocess → EDA → Feature Engineering → Model Training\n",
    "**Author:** thatlq1812\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "| Step | Description | Optional |\n",
    "|------|-------------|----------|\n",
    "| 1️ | **Configuration** - Set pipeline options | NOT Required |\n",
    "| 2️ | **Download Data** - Get latest from VM | OK Skippable |\n",
    "| 3️ | **Explore Data** - Preview raw data | OK Skippable |\n",
    "| 4️ | **Preprocess** - Convert & add features | OK Skippable |\n",
    "| 5️ | **Comprehensive EDA** - Deep analysis | OK Skippable |\n",
    "| 6️ | **Feature Engineering** - ML features | NOT Required |\n",
    "| 7️ | **Model Training** - Train & compare | NOT Required |\n",
    "| 8️ | **Save Results** - Export models | OK Skippable |\n",
    "\n",
    "---\n",
    "\n",
    "**Project:** DSP391m Traffic Forecasting - Ho Chi Minh City  \n",
    "**VM:** traffic-forecast-collector (GCP asia-southeast1-a)  \n",
    "**Coverage:** 64 intersections, 144 road segments, 4096m radius\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Start Tips:\n",
    "\n",
    "- **First time?** Run all cells with default config\n",
    "- **Have data already?** Set `USE_EXISTING_DATA = True`\n",
    "- **Quick test?** Disable EDA steps, enable `QUICK_MODE = True`\n",
    "- **Production?** Enable all steps for comprehensive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9379e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1️: Pipeline Configuration\n",
    "\n",
    "**Configure which steps to run** - Customize the pipeline to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# PIPELINE CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# ─── Data Source Options ───────────────────────────────────────────\n",
    "USE_EXISTING_DATA = True       # True: Use current data | False: Download new data\n",
    "DOWNLOAD_LATEST_ONLY = False   # True: Latest run only | False: All available runs\n",
    "USE_PREPROCESSED = False       # True: Load from Parquet | False: Process from JSON\n",
    "\n",
    "# ─── Pipeline Steps Control ────────────────────────────────────────\n",
    "ENABLE_DATA_EXPLORATION = True    # Show raw data preview\n",
    "ENABLE_PREPROCESSING = True       # Convert JSON to Parquet\n",
    "ENABLE_COMPREHENSIVE_EDA = True   # Full exploratory analysis (maps, charts)\n",
    "ENABLE_FEATURE_ENGINEERING = True # Create ML features (always needed for training)\n",
    "ENABLE_MODEL_TRAINING = True      # Train and compare models\n",
    "ENABLE_SAVE_MODELS = True         # Save trained models to disk\n",
    "\n",
    "# ─── Analysis Options ──────────────────────────────────────────────\n",
    "QUICK_MODE = False             # True: Faster execution, less detail\n",
    "SHOW_INTERACTIVE_MAPS = True   # Folium geographic visualizations\n",
    "SHOW_PLOTLY_CHARTS = True      # Interactive Plotly charts\n",
    "VERBOSE = True                 # Print detailed progress\n",
    "\n",
    "# ─── Data Paths ────────────────────────────────────────────────────\n",
    "DATA_DIR = '../data/runs'\n",
    "PROCESSED_DIR = '../data/processed'\n",
    "MODELS_DIR = '../traffic_forecast/models/saved'\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Configuration loaded!\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Pipeline Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Data Source:\")\n",
    "print(f\"   {'OK' if USE_EXISTING_DATA else 'NOT'} Use existing data (skip download)\")\n",
    "print(f\"   {'OK' if DOWNLOAD_LATEST_ONLY else 'NOT'} Download latest only\")\n",
    "print(f\"   {'OK' if USE_PREPROCESSED else 'NOT'} Use preprocessed Parquet files\")\n",
    "print(f\"\\nPipeline Steps:\")\n",
    "print(f\"   {'OK' if ENABLE_DATA_EXPLORATION else 'NOT'} Data exploration\")\n",
    "print(f\"   {'OK' if ENABLE_PREPROCESSING else 'NOT'} Preprocessing\")\n",
    "print(f\"   {'OK' if ENABLE_COMPREHENSIVE_EDA else 'NOT'} Comprehensive EDA\")\n",
    "print(f\"   {'OK' if ENABLE_FEATURE_ENGINEERING else 'NOT'} Feature engineering\")\n",
    "print(f\"   {'OK' if ENABLE_MODEL_TRAINING else 'NOT'} Model training\")\n",
    "print(f\"   {'OK' if ENABLE_SAVE_MODELS else 'NOT'} Save models\")\n",
    "print(f\"\\nAnalysis Options:\")\n",
    "print(f\"   {'OK' if QUICK_MODE else 'NOT'} Quick mode (faster)\")\n",
    "print(f\"   {'OK' if SHOW_INTERACTIVE_MAPS else 'NOT'} Interactive maps\")\n",
    "print(f\"   {'OK' if SHOW_PLOTLY_CHARTS else 'NOT'} Plotly charts\")\n",
    "print(f\"   {'OK' if VERBOSE else 'NOT'} Verbose output\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Validate configuration\n",
    "if ENABLE_MODEL_TRAINING and not ENABLE_FEATURE_ENGINEERING:\n",
    "    print(\"\\nWARNING: Model training requires feature engineering!\")\n",
    "    print(\"   Automatically enabling ENABLE_FEATURE_ENGINEERING\")\n",
    "    ENABLE_FEATURE_ENGINEERING = True\n",
    "\n",
    "if USE_PREPROCESSED and not ENABLE_PREPROCESSING:\n",
    "    print(\"\\nTIP: Using preprocessed data, skipping preprocessing step\")\n",
    "\n",
    "print(\"\\nReady to run! Execute cells below to start the pipeline.\")\n",
    "print(\"Expected: 10 runs, 1,440 records, ~6-8 minutes execution time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897bdf7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2️: Data Source Selection\n",
    "\n",
    "Choose your data source based on the configuration above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96498536",
   "metadata": {},
   "source": [
    "### Option A: Download Latest Data from VM\n",
    "\n",
    "**Run this cell only if** `USE_EXISTING_DATA = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f221193",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_EXISTING_DATA:\n",
    "    import subprocess\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"Downloading latest data from VM...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Run download script\n",
    "    result = subprocess.run(\n",
    "        ['bash', 'scripts/data/download_latest.sh'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd='..'  # Run from project root\n",
    "    )\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nOK Download completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\nFAIL Download failed!\")\n",
    "        print(result.stderr)\n",
    "else:\n",
    "    print(\"Skipping download - Using existing data\")\n",
    "    print(\"Set USE_EXISTING_DATA = False to download new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29d19d",
   "metadata": {},
   "source": [
    "### Option B: Use Existing Data\n",
    "\n",
    "Current data will be loaded from `data/runs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533547ef",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3️: Data Exploration\n",
    "\n",
    "**Preview and validate** the data we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bee095",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DATA_EXPLORATION:\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Find all runs\n",
    "    data_dir = Path(DATA_DIR)\n",
    "    run_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()], reverse=True)\n",
    "    \n",
    "    print(f\"Found {len(run_dirs)} collection runs\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display runs table\n",
    "    runs_info = []\n",
    "    for run_dir in run_dirs[:10]:  # Show latest 10\n",
    "        files = list(run_dir.glob('*.json'))\n",
    "        total_size = sum(f.stat().st_size for f in files)\n",
    "        \n",
    "        # Parse timestamp from run name\n",
    "        run_name = run_dir.name\n",
    "        if run_name.startswith('run_'):\n",
    "            timestamp_str = run_name[4:]  # Remove 'run_' prefix\n",
    "            try:\n",
    "                run_time = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')\n",
    "                time_display = run_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                time_display = timestamp_str\n",
    "        else:\n",
    "            time_display = run_name\n",
    "        \n",
    "        runs_info.append({\n",
    "            'Run': run_name,\n",
    "            'Time': time_display,\n",
    "            'Files': len(files),\n",
    "            'Size (KB)': total_size // 1024\n",
    "        })\n",
    "    \n",
    "    df_runs = pd.DataFrame(runs_info)\n",
    "    print(df_runs.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal data: {df_runs['Size (KB)'].sum():,} KB\")\n",
    "else:\n",
    "    print(\"Skipping data exploration\")\n",
    "    print(\"   Set ENABLE_DATA_EXPLORATION = True to view data details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebb27f",
   "metadata": {},
   "source": [
    "### Inspect Latest Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DATA_EXPLORATION:\n",
    "    # Load latest run\n",
    "    latest_run = run_dirs[0]\n",
    "    print(f\"Latest Run: {latest_run.name}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load all JSON files\n",
    "    files_content = {}\n",
    "    for json_file in latest_run.glob('*.json'):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            files_content[json_file.stem] = json.load(f)\n",
    "        print(f\"OK Loaded: {json_file.name}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nData Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'nodes' in files_content:\n",
    "        nodes = files_content['nodes']\n",
    "        print(f\"Nodes (Intersections): {len(nodes)}\")\n",
    "        if nodes:\n",
    "            print(f\"   Sample: {nodes[0].get('name', 'N/A')}\")\n",
    "            print(f\"   Location: ({nodes[0]['lat']:.6f}, {nodes[0]['lon']:.6f})\")\n",
    "    \n",
    "    if 'edges' in files_content:\n",
    "        edges = files_content['edges']\n",
    "        print(f\"\\nEdges (Road Segments): {len(edges)}\")\n",
    "        if edges:\n",
    "            sample = edges[0]\n",
    "            print(f\"   Sample: {sample.get('road_name', 'Unnamed road')}\")\n",
    "            print(f\"   Route: {sample.get('start_node_id', 'N/A')} → {sample.get('end_node_id', 'N/A')}\")\n",
    "            print(f\"   Distance: {sample.get('distance_m', 0):.1f} m\")\n",
    "    \n",
    "    if 'traffic_edges' in files_content:\n",
    "        traffic = files_content['traffic_edges']\n",
    "        print(f\"\\nTraffic Data: {len(traffic)} records\")\n",
    "        if traffic:\n",
    "            sample = traffic[0]\n",
    "            print(f\"   Speed: {sample.get('speed_kmh', 'N/A')} km/h\")\n",
    "            print(f\"   Duration: {sample.get('duration_sec', 'N/A')} sec\")\n",
    "            print(f\"   Distance: {sample.get('distance_km', 'N/A')} km\")\n",
    "            print(f\"   Timestamp: {sample.get('timestamp', 'N/A')}\")\n",
    "    \n",
    "    if 'weather_snapshot' in files_content:\n",
    "        weather = files_content['weather_snapshot']\n",
    "        print(f\"\\nWeather Data: {len(weather)} node records\")\n",
    "        if weather:\n",
    "            sample = weather[0]\n",
    "            print(f\"   Temperature: {sample.get('temperature_c', 'N/A')}°C\")\n",
    "            print(f\"   Wind Speed: {sample.get('wind_speed_kmh', 'N/A')} km/h\")\n",
    "            print(f\"   Precipitation: {sample.get('precipitation_mm', 'N/A')} mm\")\n",
    "    \n",
    "    if 'statistics' in files_content:\n",
    "        stats = files_content['statistics']\n",
    "        print(f\"\\nNetwork Statistics:\")\n",
    "        print(f\"   Total Nodes: {stats.get('total_nodes', 'N/A')}\")\n",
    "        print(f\"   Total Edges: {stats.get('total_edges', 'N/A')}\")\n",
    "        print(f\"   Avg Degree: {stats.get('avg_degree', 'N/A'):.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"Skipping detailed exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea7025",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4️: Data Preprocessing\n",
    "\n",
    "**Convert JSON → Parquet** for 10x faster loading + add derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_PREPROCESSING and not USE_PREPROCESSED:\n",
    "    import sys\n",
    "    sys.path.insert(0, '..')  # Add project root to path\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    \n",
    "    print(\"Preprocessing data...\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create preprocessor class (inline for notebook)\n",
    "    class SimplePreprocessor:\n",
    "        def __init__(self, data_dir=DATA_DIR, output_dir=PROCESSED_DIR):\n",
    "            self.data_dir = Path(data_dir)\n",
    "            self.output_dir = Path(output_dir)\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        def process_run(self, run_dir):\n",
    "            \"\"\"Process a single run directory\"\"\"\n",
    "            run_name = run_dir.name\n",
    "            if VERBOSE:\n",
    "                print(f\"Processing: {run_name}\")\n",
    "            \n",
    "            # Load JSON files\n",
    "            with open(run_dir / 'traffic_edges.json', 'r') as f:\n",
    "                traffic_data = json.load(f)\n",
    "            \n",
    "            with open(run_dir / 'weather_snapshot.json', 'r') as f:\n",
    "                weather_data = json.load(f)\n",
    "            \n",
    "            with open(run_dir / 'nodes.json', 'r') as f:\n",
    "                nodes_data = json.load(f)\n",
    "            \n",
    "            # Convert to DataFrames\n",
    "            df_traffic = pd.DataFrame(traffic_data)\n",
    "            df_weather = pd.DataFrame(weather_data)\n",
    "            df_nodes = pd.DataFrame(nodes_data)\n",
    "            \n",
    "            # Parse timestamp\n",
    "            df_traffic['timestamp'] = pd.to_datetime(df_traffic['timestamp'])\n",
    "            \n",
    "            # Add time-based features\n",
    "            df_traffic['hour'] = df_traffic['timestamp'].dt.hour\n",
    "            df_traffic['minute'] = df_traffic['timestamp'].dt.minute\n",
    "            df_traffic['day_of_week'] = df_traffic['timestamp'].dt.dayofweek\n",
    "            df_traffic['day_name'] = df_traffic['timestamp'].dt.day_name()\n",
    "            df_traffic['is_weekend'] = df_traffic['day_of_week'].isin([5, 6])\n",
    "            \n",
    "            # Add congestion levels based on speed\n",
    "            def categorize_congestion(speed):\n",
    "                if speed < 15:\n",
    "                    return 'heavy'\n",
    "                elif speed < 25:\n",
    "                    return 'moderate'\n",
    "                elif speed < 35:\n",
    "                    return 'light'\n",
    "                else:\n",
    "                    return 'free_flow'\n",
    "            \n",
    "            df_traffic['congestion_level'] = df_traffic['speed_kmh'].apply(categorize_congestion)\n",
    "            \n",
    "            # Add speed categories\n",
    "            df_traffic['speed_category'] = pd.cut(\n",
    "                df_traffic['speed_kmh'],\n",
    "                bins=[0, 10, 20, 30, 40, 100],\n",
    "                labels=['very_slow', 'slow', 'moderate', 'fast', 'very_fast']\n",
    "            )\n",
    "            \n",
    "            # Merge weather data (average across all nodes)\n",
    "            weather_avg = df_weather.groupby('node_id').first().reset_index()\n",
    "            avg_temp = weather_avg['temperature_c'].mean()\n",
    "            avg_wind = weather_avg['wind_speed_kmh'].mean()\n",
    "            avg_precip = weather_avg['precipitation_mm'].mean()\n",
    "            \n",
    "            df_traffic['temperature_c'] = avg_temp\n",
    "            df_traffic['wind_speed_kmh'] = avg_wind\n",
    "            df_traffic['precipitation_mm'] = avg_precip\n",
    "            \n",
    "            # Add run metadata\n",
    "            df_traffic['run_name'] = run_name\n",
    "            df_traffic['collection_time'] = df_traffic['timestamp'].iloc[0]\n",
    "            \n",
    "            # Save to Parquet\n",
    "            output_file = self.output_dir / f\"{run_name}.parquet\"\n",
    "            df_traffic.to_parquet(output_file, index=False)\n",
    "            if VERBOSE:\n",
    "                print(f\"   OK Saved: {output_file.name} ({output_file.stat().st_size // 1024} KB)\")\n",
    "            \n",
    "            return df_traffic\n",
    "        \n",
    "        def process_all(self, limit=None):\n",
    "            \"\"\"Process all runs\"\"\"\n",
    "            run_dirs = sorted([d for d in self.data_dir.iterdir() if d.is_dir()], reverse=True)\n",
    "            \n",
    "            if limit:\n",
    "                run_dirs = run_dirs[:limit]\n",
    "            \n",
    "            all_data = []\n",
    "            for run_dir in run_dirs:\n",
    "                try:\n",
    "                    df = self.process_run(run_dir)\n",
    "                    all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    if VERBOSE:\n",
    "                        print(f\"   Error: {e}\")\n",
    "            \n",
    "            # Combine all runs\n",
    "            if all_data:\n",
    "                df_combined = pd.concat(all_data, ignore_index=True)\n",
    "                combined_file = self.output_dir / 'all_runs_combined.parquet'\n",
    "                df_combined.to_parquet(combined_file, index=False)\n",
    "                print(f\"\\nCombined dataset: {combined_file.name}\")\n",
    "                print(f\"   Records: {len(df_combined):,}\")\n",
    "                print(f\"   Size: {combined_file.stat().st_size // 1024} KB\")\n",
    "                return df_combined\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    # Run preprocessing\n",
    "    preprocessor = SimplePreprocessor()\n",
    "    df_all = preprocessor.process_all()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OKPreprocessing completed!\")\n",
    "    print(f\"\\nTotal records: {len(df_all):,}\")\n",
    "    print(f\"Date range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "    print(f\"Avg speed: {df_all['speed_kmh'].mean():.2f} km/h\")\n",
    "\n",
    "elif USE_PREPROCESSED:\n",
    "    # Load from preprocessed Parquet files\n",
    "    print(\"Loading preprocessed data (fast)...\\n\")\n",
    "    combined_file = Path(PROCESSED_DIR) / 'all_runs_combined.parquet'\n",
    "    if combined_file.exists():\n",
    "        df_all = pd.read_parquet(combined_file)\n",
    "        print(f\"OKLoaded {len(df_all):,} records from {combined_file.name}\")\n",
    "        print(f\"Date range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "        print(f\"Avg speed: {df_all['speed_kmh'].mean():.2f} km/h\")\n",
    "    else:\n",
    "        print(f\"FAIL Preprocessed file not found: {combined_file}\")\n",
    "        print(\"   Set ENABLE_PREPROCESSING = True to create it\")\n",
    "else:\n",
    "    print(\"Skipping preprocessing\")\n",
    "    print(\"   Set ENABLE_PREPROCESSING = True to process data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac48dd9",
   "metadata": {},
   "source": [
    "### Preview Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d14767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Info:\\n\")\n",
    "print(df_all.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nFirst 5 rows:\\n\")\n",
    "print(df_all.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nStatistical Summary:\\n\")\n",
    "print(df_all[['speed_kmh', 'duration_sec', 'distance_km', 'temperature_c', 'wind_speed_kmh']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff5c28",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5️: Comprehensive Exploratory Data Analysis\n",
    "\n",
    "**Deep dive into patterns** - Geographic maps, correlations, time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_COMPREHENSIVE_EDA:\n",
    "    print(\"Running Comprehensive EDA...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Import visualization libraries\n",
    "    if SHOW_PLOTLY_CHARTS:\n",
    "        import plotly.express as px\n",
    "        import plotly.graph_objects as go\n",
    "        from plotly.subplots import make_subplots\n",
    "    \n",
    "    if SHOW_INTERACTIVE_MAPS:\n",
    "        import folium\n",
    "        from folium.plugins import HeatMap\n",
    "    \n",
    "    print(\"OK Visualization libraries loaded\")\n",
    "else:\n",
    "    print(\"Skipping comprehensive EDA\")\n",
    "    print(\"Set ENABLE_COMPREHENSIVE_EDA = True for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02d54e",
   "metadata": {},
   "source": [
    "### 5a. Traffic Speed Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_COMPREHENSIVE_EDA and SHOW_PLOTLY_CHARTS:\n",
    "    # Traffic Speed Analysis\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Speed Distribution', 'Speed Box Plot', \n",
    "                        'Duration Distribution', 'Speed vs Distance'),\n",
    "        specs=[[{'type': 'histogram'}, {'type': 'box'}],\n",
    "               [{'type': 'histogram'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # Speed Histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df_all['speed_kmh'], nbinsx=30, name='Speed',\n",
    "                    marker_color='steelblue', showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Speed Box Plot\n",
    "    fig.add_trace(\n",
    "        go.Box(y=df_all['speed_kmh'], name='Speed',\n",
    "               marker_color='steelblue', showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Duration Histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df_all['duration_sec'], nbinsx=30, name='Duration',\n",
    "                    marker_color='coral', showlegend=False),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Speed vs Distance Scatter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_all['distance_km'], y=df_all['speed_kmh'],\n",
    "                  mode='markers', name='Speed vs Distance',\n",
    "                  marker=dict(color='green', size=6, opacity=0.4),\n",
    "                  showlegend=False),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Speed (km/h)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Speed (km/h)\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Duration (seconds)\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Distance (km)\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Speed (km/h)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Speed (km/h)\", row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(height=700, title_text=\"Traffic Speed & Duration Analysis\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Print congestion statistics\n",
    "    print(\"\\nCongestion Level Distribution:\")\n",
    "    print(df_all['congestion_level'].value_counts().sort_index())\n",
    "    print(f\"\\nAverage Speed: {df_all['speed_kmh'].mean():.2f} km/h\")\n",
    "    print(f\"Average Duration: {df_all['duration_sec'].mean():.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cf74e",
   "metadata": {},
   "source": [
    "### 5b. Hourly Traffic Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_COMPREHENSIVE_EDA:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Hourly patterns\n",
    "    hourly_speed = df_all.groupby('hour')['speed_kmh'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    fig.suptitle('Hourly Traffic Patterns', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Average speed by hour\n",
    "    axes[0].plot(hourly_speed['hour'], hourly_speed['mean'], marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "    axes[0].fill_between(\n",
    "        hourly_speed['hour'],\n",
    "        hourly_speed['mean'] - hourly_speed['std'],\n",
    "        hourly_speed['mean'] + hourly_speed['std'],\n",
    "        alpha=0.3,\n",
    "        color='steelblue'\n",
    "    )\n",
    "    axes[0].set_xlabel('Hour of Day', fontsize=12)\n",
    "    axes[0].set_ylabel('Average Speed (km/h)', fontsize=12)\n",
    "    axes[0].set_title('Average Speed by Hour (with std deviation)')\n",
    "    axes[0].set_xticks(range(0, 24, 2))\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=df_all['speed_kmh'].mean(), color='red', linestyle='--', label='Overall Average')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Traffic volume by hour\n",
    "    axes[1].bar(hourly_speed['hour'], hourly_speed['count'], color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Hour of Day', fontsize=12)\n",
    "    axes[1].set_ylabel('Number of Records', fontsize=12)\n",
    "    axes[1].set_title('Traffic Data Volume by Hour')\n",
    "    axes[1].set_xticks(range(0, 24, 2))\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Peak hours identification\n",
    "    peak_hours = hourly_speed.nsmallest(3, 'mean')[['hour', 'mean']]\n",
    "    print(\"\\nSlowest Hours (Peak Congestion):\")\n",
    "    for _, row in peak_hours.iterrows():\n",
    "        print(f\"   • Hour {int(row['hour']):02d}:00 - Avg Speed: {row['mean']:.2f} km/h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cddffb",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6️: Feature Engineering for ML\n",
    "\n",
    "**Create advanced features** for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99328b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_FEATURE_ENGINEERING:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    print(\"Engineering features...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create edge_id from node pairs (if not exists)\n",
    "    if 'edge_id' not in df_all.columns:\n",
    "        df_all['edge_id'] = df_all['node_a_id'] + '_to_' + df_all['node_b_id']\n",
    "        print(\"✓ Created edge_id from node_a_id and node_b_id\")\n",
    "    \n",
    "    # Sort by timestamp for lag features\n",
    "    df_all = df_all.sort_values(['edge_id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    # 1. Lag features (previous speeds)\n",
    "    print(\"\\n1️ Creating lag features...\")\n",
    "    for lag in [1, 2, 3]:\n",
    "        df_all[f'speed_lag_{lag}'] = df_all.groupby('edge_id')['speed_kmh'].shift(lag)\n",
    "    print(f\"   OK Added: speed_lag_1, speed_lag_2, speed_lag_3\")\n",
    "    \n",
    "    # 2. Rolling statistics\n",
    "    print(\"\\n2️ Creating rolling statistics...\")\n",
    "    df_all['speed_rolling_mean_3'] = df_all.groupby('edge_id')['speed_kmh'].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "    df_all['speed_rolling_std_3'] = df_all.groupby('edge_id')['speed_kmh'].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).std()\n",
    "    )\n",
    "    print(f\"   OK Added: speed_rolling_mean_3, speed_rolling_std_3\")\n",
    "    \n",
    "    # 3. Cyclical time features\n",
    "    print(\"\\n3️ Creating cyclical time features...\")\n",
    "    df_all['hour_sin'] = np.sin(2 * np.pi * df_all['hour'] / 24)\n",
    "    df_all['hour_cos'] = np.cos(2 * np.pi * df_all['hour'] / 24)\n",
    "    df_all['day_of_week_sin'] = np.sin(2 * np.pi * df_all['day_of_week'] / 7)\n",
    "    df_all['day_of_week_cos'] = np.cos(2 * np.pi * df_all['day_of_week'] / 7)\n",
    "    print(f\"   OK Added: hour_sin, hour_cos, day_of_week_sin, day_of_week_cos\")\n",
    "    \n",
    "    # 4. Rush hour indicators\n",
    "    print(\"\\n4️ Creating rush hour indicators...\")\n",
    "    df_all['is_morning_rush'] = df_all['hour'].isin([7, 8, 9]).astype(int)\n",
    "    df_all['is_evening_rush'] = df_all['hour'].isin([17, 18, 19]).astype(int)\n",
    "    df_all['is_rush_hour'] = (df_all['is_morning_rush'] | df_all['is_evening_rush']).astype(int)\n",
    "    print(f\"   OK Added: is_morning_rush, is_evening_rush, is_rush_hour\")\n",
    "    \n",
    "    # 5. Encode categorical features\n",
    "    print(\"\\n5️ Encoding categorical features...\")\n",
    "    le_congestion = LabelEncoder()\n",
    "    df_all['congestion_level_encoded'] = le_congestion.fit_transform(df_all['congestion_level'])\n",
    "    print(f\"   OK Added: congestion_level_encoded\")\n",
    "    \n",
    "    # Drop rows with NaN (from lag features)\n",
    "    df_features = df_all.dropna().copy()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"OKFeature engineering completed!\")\n",
    "    print(f\"\\nDataset shape: {df_features.shape}\")\n",
    "    print(f\"Total features: {df_features.shape[1]}\")\n",
    "    print(f\"Dropped {len(df_all) - len(df_features)} rows with missing lag values\")\n",
    "else:\n",
    "    print(\"Skipping feature engineering\")\n",
    "    print(\"WARNING: Required for model training!\")\n",
    "    df_features = df_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575fb1b",
   "metadata": {},
   "source": [
    "### Feature List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd317c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Features:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "feature_groups = {\n",
    "    'Traffic Features': ['speed_kmh', 'duration_sec', 'distance_km', 'congestion_level', 'speed_category'],\n",
    "    'Time Features': ['hour', 'minute', 'day_of_week', 'day_name', 'is_weekend', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos'],\n",
    "    'Weather Features': ['temperature_c', 'wind_speed_kmh', 'precipitation_mm'],\n",
    "    'Lag Features': ['speed_lag_1', 'speed_lag_2', 'speed_lag_3'],\n",
    "    'Rolling Features': ['speed_rolling_mean_3', 'speed_rolling_std_3'],\n",
    "    'Rush Hour Features': ['is_morning_rush', 'is_evening_rush', 'is_rush_hour'],\n",
    "    'Encoded Features': ['congestion_level_encoded']\n",
    "}\n",
    "\n",
    "for group_name, features in feature_groups.items():\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    for feat in features:\n",
    "        if feat in df_features.columns:\n",
    "            print(f\"   OK {feat}\")\n",
    "        else:\n",
    "            print(f\"   NOT {feat} (missing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42517a9a",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7️: Train & Compare Models\n",
    "\n",
    "**Train multiple models** and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    print(\"Splitting data...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Define features for modeling\n",
    "    feature_columns = [\n",
    "        # Time features\n",
    "        'hour', 'day_of_week', 'is_weekend', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "        # Traffic features\n",
    "        'distance_km', 'duration_sec',\n",
    "        # Weather features\n",
    "        'temperature_c', 'wind_speed_kmh', 'precipitation_mm',\n",
    "        # Lag features\n",
    "        'speed_lag_1', 'speed_lag_2', 'speed_lag_3',\n",
    "        # Rolling features\n",
    "        'speed_rolling_mean_3', 'speed_rolling_std_3',\n",
    "        # Rush hour\n",
    "        'is_morning_rush', 'is_evening_rush', 'is_rush_hour'\n",
    "    ]\n",
    "    \n",
    "    target_column = 'speed_kmh'\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df_features[feature_columns].copy()\n",
    "    y = df_features[target_column].copy()\n",
    "    \n",
    "    # Time-based split (last 20% as test)\n",
    "    split_idx = int(len(df_features) * 0.8)\n",
    "    X_train = X.iloc[:split_idx]\n",
    "    X_test = X.iloc[split_idx:]\n",
    "    y_train = y.iloc[:split_idx]\n",
    "    y_test = y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Dataset split (time-based):\") \n",
    "    print(f\"   Training set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    print(f\"\\nFeatures: {len(feature_columns)}\")\n",
    "    print(f\"Target: {target_column}\")\n",
    "else:\n",
    "    print(\"Skipping train-test split\")\n",
    "    print(\"   Set ENABLE_MODEL_TRAINING = True to train models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e13c3a7",
   "metadata": {},
   "source": [
    "### Model Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING:\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import LinearRegression, Ridge\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    import time\n",
    "    \n",
    "    print(\"Training models...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Define models\n",
    "    if QUICK_MODE:\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1),\n",
    "        }\n",
    "        print(\"Quick Mode: Training 2 fast models\\n\")\n",
    "    else:\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge Regression': Ridge(alpha=1.0),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "        }\n",
    "        print(\"Full Mode: Training 4 models\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training: {model_name}\")\n",
    "        \n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Train Time (s)': train_time\n",
    "        })\n",
    "        \n",
    "        print(f\"   OK MAE: {mae:.3f} km/h\")\n",
    "        print(f\"   OK RMSE: {rmse:.3f} km/h\")\n",
    "        print(f\"   OK R²: {r2:.3f}\")\n",
    "        print(f\"   Time: {train_time:.2f}s\\n\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"OKAll models trained!\")\n",
    "else:\n",
    "    print(\"Skipping model training\")\n",
    "    print(\"Set ENABLE_MODEL_TRAINING = True to train models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d750f6",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING:\n",
    "    # Create comparison table\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results.sort_values('RMSE')\n",
    "    \n",
    "    print(\"\\nModel Performance Comparison:\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    best_model_name = df_results.iloc[0]['Model']\n",
    "    best_rmse = df_results.iloc[0]['RMSE']\n",
    "    best_r2 = df_results.iloc[0]['R²']\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    print(f\"   RMSE: {best_rmse:.3f} km/h\")\n",
    "    print(f\"   R²: {best_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1655fb",
   "metadata": {},
   "source": [
    "### Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be646728",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_TRAINING:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('Model Performance Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # MAE comparison\n",
    "    axes[0].barh(df_results['Model'], df_results['MAE'], color='skyblue')\n",
    "    axes[0].set_xlabel('MAE (km/h) - Lower is Better')\n",
    "    axes[0].set_title('Mean Absolute Error')\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # RMSE comparison\n",
    "    axes[1].barh(df_results['Model'], df_results['RMSE'], color='lightcoral')\n",
    "    axes[1].set_xlabel('RMSE (km/h) - Lower is Better')\n",
    "    axes[1].set_title('Root Mean Squared Error')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # R² comparison\n",
    "    axes[2].barh(df_results['Model'], df_results['R²'], color='lightgreen')\n",
    "    axes[2].set_xlabel('R² Score - Higher is Better')\n",
    "    axes[2].set_title('R² Score')\n",
    "    axes[2].set_xlim(0, 1)\n",
    "    axes[2].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5b168",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8️: Save Models & Results\n",
    "\n",
    "**Export trained models** for deployment and future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9066e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_SAVE_MODELS and ENABLE_MODEL_TRAINING:\n",
    "    import joblib\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Create models directory\n",
    "    models_dir = Path(MODELS_DIR)\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"Saving models...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Save all models\n",
    "    for model_name, model in models.items():\n",
    "        filename = model_name.lower().replace(' ', '_') + '.pkl'\n",
    "        filepath = models_dir / filename\n",
    "        joblib.dump(model, filepath)\n",
    "        print(f\"OK Saved: {filename} ({filepath.stat().st_size // 1024} KB)\")\n",
    "    \n",
    "    # Save feature columns\n",
    "    feature_info = {\n",
    "        'feature_columns': feature_columns,\n",
    "        'target_column': target_column,\n",
    "        'num_features': len(feature_columns)\n",
    "    }\n",
    "    joblib.dump(feature_info, models_dir / 'feature_info.pkl')\n",
    "    print(f\"\\nOK Saved feature info\")\n",
    "    \n",
    "    # Save results\n",
    "    df_results.to_csv(models_dir / 'model_comparison.csv', index=False)\n",
    "    print(f\"OK Saved model comparison\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"OKAll artifacts saved to: {models_dir}\")\n",
    "elif not ENABLE_MODEL_TRAINING:\n",
    "    print(\"No models to save (training was skipped)\")\n",
    "else:\n",
    "    print(\"Skipping model saving\")\n",
    "    print(\"   Set ENABLE_SAVE_MODELS = True to save models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289eccad",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Complete!\n",
    "\n",
    "**Summary of execution** and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5374530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"╔\" + \"═\" * 68 + \"╗\")\n",
    "print(\"║\" + \" \" * 20 + \"PIPELINE EXECUTION COMPLETE!\" + \" \" * 17 + \"║\")\n",
    "print(\"╚\" + \"═\" * 68 + \"╝\")\n",
    "print()\n",
    "\n",
    "# Summary\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "steps_summary = [\n",
    "    (\"Configuration\", True, \"OKLoaded\"),\n",
    "    (\"Download Data\", not USE_EXISTING_DATA, \"OKDownloaded\" if not USE_EXISTING_DATA else \"Skipped (used existing)\"),\n",
    "    (\"Data Exploration\", ENABLE_DATA_EXPLORATION, \"OKExplored\" if ENABLE_DATA_EXPLORATION else \"Skipped\"),\n",
    "    (\"Preprocessing\", ENABLE_PREPROCESSING, \"OKProcessed\" if ENABLE_PREPROCESSING else \"Skipped\"),\n",
    "    (\"Comprehensive EDA\", ENABLE_COMPREHENSIVE_EDA, \"OKAnalyzed\" if ENABLE_COMPREHENSIVE_EDA else \"Skipped\"),\n",
    "    (\"Feature Engineering\", ENABLE_FEATURE_ENGINEERING, f\"OKCreated {len(feature_columns)} features\" if ENABLE_FEATURE_ENGINEERING else \"Skipped\"),\n",
    "    (\"Model Training\", ENABLE_MODEL_TRAINING, f\"OKTrained {len(models)} models\" if ENABLE_MODEL_TRAINING else \"Skipped\"),\n",
    "    (\"Save Models\", ENABLE_SAVE_MODELS and ENABLE_MODEL_TRAINING, \"OKSaved\" if ENABLE_SAVE_MODELS and ENABLE_MODEL_TRAINING else \"Skipped\"),\n",
    "]\n",
    "\n",
    "for step_name, executed, status in steps_summary:\n",
    "    print(f\"• {step_name:.<25} {status}\")\n",
    "\n",
    "if ENABLE_MODEL_TRAINING:\n",
    "    print(\"\\nBEST MODEL RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"• Model: {best_model_name}\")\n",
    "    print(f\"• RMSE: {best_rmse:.3f} km/h\")\n",
    "    print(f\"• MAE: {df_results.iloc[0]['MAE']:.3f} km/h\")\n",
    "    print(f\"• R² Score: {best_r2:.3f}\")\n",
    "    print(f\"• Training Time: {df_results.iloc[0]['Train Time (s)']:.2f}s\")\n",
    "\n",
    "print(\"\\nNEXT STEPS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Hyperparameter Tuning - Optimize best model with GridSearch\")\n",
    "print(\"2. Advanced Models - Try LSTM, GNN for temporal/spatial patterns\")\n",
    "print(\"3. Ensemble Methods - Combine multiple models\")\n",
    "print(\"4. Deploy API - Create endpoint for real-time predictions\")\n",
    "print(\"5. Dashboard - Build real-time monitoring dashboard\")\n",
    "\n",
    "print(\"\\nTO RE-RUN THIS PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"• Just 'Run All Cells' again!\")\n",
    "print(\"• Or adjust configuration in Step 1 and re-run\")\n",
    "print(\"• Set USE_EXISTING_DATA = False to download fresh data\")\n",
    "print(\"• Enable/disable steps as needed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Thank you for using the Traffic Forecasting Pipeline!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
