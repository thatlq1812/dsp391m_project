{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b53ffed",
   "metadata": {},
   "source": [
    "# üéõÔ∏è TRAFFIC FORECAST - CONTROL PANEL v5.1\n",
    "\n",
    "**Project:** Traffic Forecast Academic (DSP391m)  \n",
    "**Version:** 5.1 - Adaptive Scheduling & Smart Caching  \n",
    "**Coverage:** HCMC Downtown, 4096m radius, 78 nodes  \n",
    "**Cost:** $21/day (25% savings), $147 for 7 days  \n",
    "**Updated:** October 29, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Pipeline Overview\n",
    "\n",
    "This notebook controls the **entire data collection pipeline** for deployment:\n",
    "\n",
    "1. **‚öôÔ∏è Configuration** - Update configs, verify settings\n",
    "2. **üó∫Ô∏è Node Topology** - Manage Overpass cache, view node distribution  \n",
    "3. **üì° Data Collection** - Run Google API collection, monitor progress\n",
    "4. **‚úÖ Data Quality** - Validate collected data, check completeness\n",
    "5. **üìä Visualization** - Plot traffic patterns, node coverage\n",
    "6. **üöÄ Deployment** - Deploy to GCP VM, schedule collection\n",
    "7. **üëÄ Monitoring** - Check logs, API costs, system health\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887bf2b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üéõÔ∏è TRAFFIC FORECAST CONTROL PANEL v5.0\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÅ Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
    "print(f\"üìÖ Current Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8503d70",
   "metadata": {},
   "source": [
    "### Check Required Files & Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check critical files\n",
    "required_files = {\n",
    "    '‚öôÔ∏è Config': 'configs/project_config.yaml',\n",
    "    'üîë API Keys': '.env',\n",
    "    'üó∫Ô∏è Overpass Cache': 'cache/overpass_topology.json',\n",
    "    'üìä Statistics': 'data/statistics.json',\n",
    "}\n",
    "\n",
    "print(\"\\nüìã FILE CHECK:\")\n",
    "print(\"-\" * 70)\n",
    "for name, path in required_files.items():\n",
    "    file_path = PROJECT_ROOT / path\n",
    "    status = \"‚úÖ\" if file_path.exists() else \"‚ùå\"\n",
    "    size = f\"{file_path.stat().st_size:,} bytes\" if file_path.exists() else \"N/A\"\n",
    "    print(f\"{status} {name:20s} {path:40s} {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2a863",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Configuration Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09046d77",
   "metadata": {},
   "source": [
    "### View Current Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20479c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display config\n",
    "config_file = PROJECT_ROOT / 'configs' / 'project_config.yaml'\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\n‚öôÔ∏è CURRENT CONFIGURATION:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display key settings\n",
    "sections = {\n",
    "    'area': ['name', 'center_lat', 'center_lon', 'radius_m'],\n",
    "    'node_selection': ['min_degree', 'min_importance', 'min_distance_meters', 'road_types'],\n",
    "    'collection': ['k_nearest', 'batch_size'],\n",
    "    'google_api': ['rate_limit_per_minute', 'retry_attempts']\n",
    "}\n",
    "\n",
    "for section, keys in sections.items():\n",
    "    print(f\"\\nüìå {section.upper().replace('_', ' ')}:\")\n",
    "    for key in keys:\n",
    "        value = config.get(section, {}).get(key)\n",
    "        print(f\"   ‚Ä¢ {key:25s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58155de8",
   "metadata": {},
   "source": [
    "### Update Configuration (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefdf289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_config(section, key, new_value):\n",
    "    \"\"\"Update a configuration value\"\"\"\n",
    "    config_file = PROJECT_ROOT / 'configs' / 'project_config.yaml'\n",
    "    \n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    if section not in config:\n",
    "        config[section] = {}\n",
    "    \n",
    "    old_value = config[section].get(key)\n",
    "    config[section][key] = new_value\n",
    "    \n",
    "    with open(config_file, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    print(f\"‚úÖ Updated {section}.{key}: {old_value} ‚Üí {new_value}\")\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# update_config('collection', 'batch_size', 10)\n",
    "# update_config('google_api', 'rate_limit_per_minute', 2800)\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use update_config(section, key, value) to change settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44af51c",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Node Topology Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933542fb",
   "metadata": {},
   "source": [
    "### View Cached Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec45966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze topology\n",
    "cache_file = PROJECT_ROOT / 'cache' / 'overpass_topology.json'\n",
    "stats_file = PROJECT_ROOT / 'data' / 'statistics.json'\n",
    "\n",
    "if cache_file.exists():\n",
    "    with open(cache_file, 'r') as f:\n",
    "        topology = json.load(f)\n",
    "    \n",
    "    nodes = topology.get('nodes', [])\n",
    "    edges = topology.get('edges', [])\n",
    "    \n",
    "    print(\"\\nüó∫Ô∏è CACHED TOPOLOGY:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìç Total Nodes: {len(nodes)}\")\n",
    "    print(f\"üîó Total Edges: {len(edges)}\")\n",
    "    \n",
    "    # Load statistics\n",
    "    if stats_file.exists():\n",
    "        with open(stats_file, 'r') as f:\n",
    "            stats = json.load(f)\n",
    "        \n",
    "        print(f\"\\nüìä NODE STATISTICS:\")\n",
    "        print(f\"   ‚Ä¢ Average Degree: {stats.get('avg_degree', 0):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Degree Range: {stats.get('min_degree', 0)} - {stats.get('max_degree', 0)}\")\n",
    "        print(f\"   ‚Ä¢ Average Importance: {stats.get('avg_importance', 0):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Importance Range: {stats.get('min_importance', 0):.1f} - {stats.get('max_importance', 0):.1f}\")\n",
    "        print(f\"   ‚Ä¢ Min Distance: {stats.get('min_distance_meters', 0)}m\")\n",
    "        \n",
    "        print(f\"\\nüö¶ ROAD TYPE DISTRIBUTION:\")\n",
    "        for road_type, count in stats.get('road_type_distribution', {}).items():\n",
    "            pct = (count / len(nodes) * 100) if nodes else 0\n",
    "            print(f\"   ‚Ä¢ {road_type:15s}: {count:3d} nodes ({pct:5.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ùå No cached topology found. Run Overpass collection first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3ce05",
   "metadata": {},
   "source": [
    "### Refresh Topology Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_topology(force=False):\n",
    "    \"\"\"Refresh Overpass topology cache\"\"\"\n",
    "    cache_file = PROJECT_ROOT / 'cache' / 'overpass_topology.json'\n",
    "    \n",
    "    if cache_file.exists() and not force:\n",
    "        print(f\"‚ÑπÔ∏è Cache exists. Use refresh_topology(force=True) to force refresh.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîÑ Refreshing topology from Overpass API...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    cmd = [\n",
    "        'conda', 'run', '-n', 'dsp', '--no-capture-output',\n",
    "        'python', 'traffic_forecast/collectors/overpass/collector.py'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Topology refresh completed successfully!\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå Topology refresh failed!\")\n",
    "        print(result.stderr)\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# refresh_topology(force=False)\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use refresh_topology(force=True) to update node cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeefc44",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b7722",
   "metadata": {},
   "source": [
    "### Check API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099eb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check .env file\n",
    "env_file = PROJECT_ROOT / '.env'\n",
    "\n",
    "if env_file.exists():\n",
    "    with open(env_file, 'r') as f:\n",
    "        env_content = f.read()\n",
    "    \n",
    "    # Extract API key (masked)\n",
    "    for line in env_content.split('\\n'):\n",
    "        if line.startswith('GOOGLE_MAPS_API_KEY='):\n",
    "            api_key = line.split('=')[1].strip()\n",
    "            masked_key = api_key[:20] + '...' + api_key[-4:] if len(api_key) > 24 else 'INVALID'\n",
    "            print(f\"\\nüîë Google Maps API Key: {masked_key}\")\n",
    "            print(f\"   Status: {'‚úÖ Set' if api_key else '‚ùå Missing'}\")\n",
    "            break\n",
    "else:\n",
    "    print(\"‚ùå .env file not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51f8c5",
   "metadata": {},
   "source": [
    "### Run Collection (Test Mode - 5 edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_collection_test():\n",
    "    \"\"\"Run test collection with 5 edges\"\"\"\n",
    "    print(\"\\nüß™ TEST COLLECTION (5 edges):\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    cmd = [\n",
    "        'bash', '-c',\n",
    "        'export GOOGLE_TEST_LIMIT=5 && conda run -n dsp --no-capture-output python traffic_forecast/collectors/google/collector.py'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr)\n",
    "    \n",
    "    return result.returncode == 0\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# success = run_collection_test()\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use run_collection_test() to test with 5 edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843458f",
   "metadata": {},
   "source": [
    "### Run Full Collection (All Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8096e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_collection_full():\n",
    "    \"\"\"Run full collection with all edges\"\"\"\n",
    "    print(\"\\nüöÄ FULL COLLECTION (all edges):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚è≥ This may take several minutes...\")\n",
    "    \n",
    "    cmd = [\n",
    "        'conda', 'run', '-n', 'dsp', '--no-capture-output',\n",
    "        'python', 'traffic_forecast/collectors/google/collector.py'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr)\n",
    "    \n",
    "    # Parse summary\n",
    "    if 'Success rate:' in result.stdout:\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'Total edges:' in line or 'Successful:' in line or 'Failed:' in line or 'Success rate:' in line:\n",
    "                print(f\"   {line.strip()}\")\n",
    "    \n",
    "    return result.returncode == 0\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# success = run_collection_full()\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use run_collection_full() to collect all edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7facfdd",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Data Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f085745",
   "metadata": {},
   "source": [
    "### Validate Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_collection():\n",
    "    \"\"\"Validate the most recent collection\"\"\"\n",
    "    traffic_file = PROJECT_ROOT / 'data' / 'traffic_edges.json'\n",
    "    \n",
    "    if not traffic_file.exists():\n",
    "        print(\"‚ùå No traffic data found. Run collection first.\")\n",
    "        return False\n",
    "    \n",
    "    with open(traffic_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"\\n‚úÖ DATA VALIDATION:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üì¶ Total Records: {len(data)}\")\n",
    "    \n",
    "    if not data:\n",
    "        print(\"‚ùå No data in file!\")\n",
    "        return False\n",
    "    \n",
    "    # Check data structure\n",
    "    sample = data[0]\n",
    "    required_fields = ['origin', 'destination', 'speed_kmh', 'duration_sec', 'distance_km', 'timestamp']\n",
    "    \n",
    "    print(f\"\\nüìã Field Coverage:\")\n",
    "    for field in required_fields:\n",
    "        present = field in sample\n",
    "        status = \"‚úÖ\" if present else \"‚ùå\"\n",
    "        print(f\"   {status} {field}\")\n",
    "    \n",
    "    # Data statistics\n",
    "    speeds = [d.get('speed_kmh', 0) for d in data if 'speed_kmh' in d]\n",
    "    \n",
    "    if speeds:\n",
    "        import statistics\n",
    "        print(f\"\\nüöó Speed Statistics (km/h):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {statistics.mean(speeds):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {statistics.median(speeds):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Min: {min(speeds):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {max(speeds):.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {statistics.stdev(speeds):.2f}\" if len(speeds) > 1 else \"\")\n",
    "    \n",
    "    # Timestamp check\n",
    "    timestamps = [d.get('timestamp') for d in data if 'timestamp' in d]\n",
    "    if timestamps:\n",
    "        first_time = min(timestamps)\n",
    "        last_time = max(timestamps)\n",
    "        print(f\"\\n‚è∞ Time Range:\")\n",
    "        print(f\"   ‚Ä¢ First: {first_time}\")\n",
    "        print(f\"   ‚Ä¢ Last: {last_time}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# validate_collection()\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use validate_collection() to check data quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d749e89",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc35880",
   "metadata": {},
   "source": [
    "### Plot Node Coverage Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92685625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_node_coverage():\n",
    "    \"\"\"Plot node coverage on map\"\"\"\n",
    "    cache_file = PROJECT_ROOT / 'cache' / 'overpass_topology.json'\n",
    "    \n",
    "    if not cache_file.exists():\n",
    "        print(\"‚ùå No topology cache found\")\n",
    "        return\n",
    "    \n",
    "    with open(cache_file, 'r') as f:\n",
    "        topology = json.load(f)\n",
    "    \n",
    "    nodes = topology.get('nodes', [])\n",
    "    \n",
    "    if not nodes:\n",
    "        print(\"‚ùå No nodes in cache\")\n",
    "        return\n",
    "    \n",
    "    # Extract coordinates\n",
    "    lats = [n['lat'] for n in nodes]\n",
    "    lons = [n['lon'] for n in nodes]\n",
    "    \n",
    "    # Load config for area center\n",
    "    config_file = PROJECT_ROOT / 'configs' / 'project_config.yaml'\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    center_lat = config['area']['center_lat']\n",
    "    center_lon = config['area']['center_lon']\n",
    "    radius_m = config['area']['radius_m']\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot nodes\n",
    "    plt.scatter(lons, lats, c='red', s=50, alpha=0.6, label=f'Nodes ({len(nodes)})', zorder=3)\n",
    "    \n",
    "    # Plot center\n",
    "    plt.scatter([center_lon], [center_lat], c='blue', s=200, marker='*', \n",
    "                label='Center', zorder=4, edgecolors='black')\n",
    "    \n",
    "    # Plot radius circle\n",
    "    radius_deg = radius_m / 111000  # rough conversion\n",
    "    circle = plt.Circle((center_lon, center_lat), radius_deg, \n",
    "                        color='blue', fill=False, linestyle='--', \n",
    "                        linewidth=2, label=f'Coverage ({radius_m}m)', zorder=2)\n",
    "    plt.gca().add_patch(circle)\n",
    "    \n",
    "    plt.xlabel('Longitude', fontsize=12)\n",
    "    plt.ylabel('Latitude', fontsize=12)\n",
    "    plt.title(f'Node Coverage Map - {config[\"area\"][\"name\"]}', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Plotted {len(nodes)} nodes\")\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# plot_node_coverage()\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use plot_node_coverage() to visualize nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116dac28",
   "metadata": {},
   "source": [
    "### Plot Traffic Speed Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speed_distribution():\n",
    "    \"\"\"Plot traffic speed distribution\"\"\"\n",
    "    traffic_file = PROJECT_ROOT / 'data' / 'traffic_edges.json'\n",
    "    \n",
    "    if not traffic_file.exists():\n",
    "        print(\"‚ùå No traffic data found\")\n",
    "        return\n",
    "    \n",
    "    with open(traffic_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    speeds = [d.get('speed_kmh', 0) for d in data if 'speed_kmh' in d]\n",
    "    \n",
    "    if not speeds:\n",
    "        print(\"‚ùå No speed data available\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(speeds, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Speed (km/h)', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title('Traffic Speed Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.axvline(np.mean(speeds), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(speeds):.1f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(speeds, vert=True)\n",
    "    plt.ylabel('Speed (km/h)', fontsize=12)\n",
    "    plt.title('Traffic Speed Box Plot', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Plotted {len(speeds)} speed measurements\")\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# plot_speed_distribution()\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use plot_speed_distribution() to visualize traffic speeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece6d6b",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ Deployment to GCP VM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223c061",
   "metadata": {},
   "source": [
    "### Generate Deployment Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deployment_package():\n",
    "    \"\"\"Create deployment package for GCP VM\"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    deploy_dir = PROJECT_ROOT / 'deploy_package'\n",
    "    deploy_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\nüì¶ CREATING DEPLOYMENT PACKAGE:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Files to include\n",
    "    files_to_copy = [\n",
    "        'configs/project_config.yaml',\n",
    "        'cache/overpass_topology.json',\n",
    "        '.env',\n",
    "        'traffic_forecast/collectors/google/collector.py',\n",
    "        'traffic_forecast/collectors/overpass/collector.py',\n",
    "        'requirements.txt',\n",
    "    ]\n",
    "    \n",
    "    for file_path in files_to_copy:\n",
    "        src = PROJECT_ROOT / file_path\n",
    "        if src.exists():\n",
    "            dst = deploy_dir / file_path\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"‚úÖ Copied: {file_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Missing: {file_path}\")\n",
    "    \n",
    "    # Create deployment script\n",
    "    deploy_script = deploy_dir / 'deploy.sh'\n",
    "    with open(deploy_script, 'w') as f:\n",
    "        f.write(\"\"\"#!/bin/bash\n",
    "# Deployment script for GCP VM\n",
    "\n",
    "echo \"üöÄ Deploying Traffic Forecast v5.0...\"\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Run Overpass collection (one-time)\n",
    "python traffic_forecast/collectors/overpass/collector.py\n",
    "\n",
    "# Run Google collection\n",
    "python traffic_forecast/collectors/google/collector.py\n",
    "\n",
    "echo \"‚úÖ Deployment complete!\"\n",
    "\"\"\")\n",
    "    \n",
    "    deploy_script.chmod(0o755)\n",
    "    print(f\"‚úÖ Created deployment script\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Package created at: {deploy_dir}\")\n",
    "    print(f\"üìã Total files: {sum(1 for _ in deploy_dir.rglob('*') if _.is_file())}\")\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# create_deployment_package()\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use create_deployment_package() to prepare deployment files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7a9dc",
   "metadata": {},
   "source": [
    "### Scheduled Collection Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3900267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cron_script():\n",
    "    \"\"\"Create cron script for scheduled collection\"\"\"\n",
    "    script_file = PROJECT_ROOT / 'scripts' / 'run_scheduled_collection.sh'\n",
    "    \n",
    "    script_content = \"\"\"#!/bin/bash\n",
    "# Scheduled collection script for cron\n",
    "# Add to crontab: */15 * * * * /path/to/run_scheduled_collection.sh\n",
    "\n",
    "cd \"$(dirname \"$0\")/..\"\n",
    "\n",
    "# Activate conda environment\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate dsp\n",
    "\n",
    "# Run collection\n",
    "python traffic_forecast/collectors/google/collector.py >> logs/collection.log 2>&1\n",
    "\n",
    "# Log completion\n",
    "echo \"[$(date)] Collection completed\" >> logs/cron.log\n",
    "\"\"\"\n",
    "    \n",
    "    with open(script_file, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    script_file.chmod(0o755)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created scheduled collection script:\")\n",
    "    print(f\"   {script_file}\")\n",
    "    print(f\"\\nüìù To schedule every 15 minutes, add to crontab:\")\n",
    "    print(f\"   */15 * * * * {script_file}\")\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# create_cron_script()\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use create_cron_script() to generate cron script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36193ded",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Monitoring & Cost Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9c215",
   "metadata": {},
   "source": [
    "### Estimate API Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_api_costs(collections_per_day=96):\n",
    "    \"\"\"\n",
    "    Estimate Google Maps API costs\n",
    "    \n",
    "    Args:\n",
    "        collections_per_day: Number of collection runs per day (default: 96 = every 15 min)\n",
    "    \"\"\"\n",
    "    cache_file = PROJECT_ROOT / 'cache' / 'overpass_topology.json'\n",
    "    \n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, 'r') as f:\n",
    "            topology = json.load(f)\n",
    "        nodes = len(topology.get('nodes', []))\n",
    "    else:\n",
    "        nodes = 78  # default\n",
    "    \n",
    "    # Calculate costs\n",
    "    edges_per_collection = nodes * 3  # k_nearest = 3\n",
    "    cost_per_request = 0.005  # $5 per 1000 requests\n",
    "    \n",
    "    print(\"\\nüí∞ API COST ESTIMATION:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìç Nodes: {nodes}\")\n",
    "    print(f\"üîó Edges per collection: {edges_per_collection}\")\n",
    "    print(f\"üîÑ Collections per day: {collections_per_day}\")\n",
    "    print(f\"\\nüìä COSTS:\")\n",
    "    \n",
    "    # Daily\n",
    "    daily_requests = edges_per_collection * collections_per_day\n",
    "    daily_cost = daily_requests * cost_per_request\n",
    "    print(f\"   ‚Ä¢ Daily: {daily_requests:,} requests = ${daily_cost:.2f}\")\n",
    "    \n",
    "    # Weekly\n",
    "    weekly_requests = daily_requests * 7\n",
    "    weekly_cost = daily_cost * 7\n",
    "    print(f\"   ‚Ä¢ Weekly: {weekly_requests:,} requests = ${weekly_cost:.2f}\")\n",
    "    \n",
    "    # Monthly\n",
    "    monthly_requests = daily_requests * 30\n",
    "    monthly_cost = daily_cost * 30\n",
    "    print(f\"   ‚Ä¢ Monthly: {monthly_requests:,} requests = ${monthly_cost:.2f}\")\n",
    "    \n",
    "    # Free tier\n",
    "    free_tier = 200  # $200 free credit\n",
    "    free_days = free_tier / daily_cost if daily_cost > 0 else 0\n",
    "    print(f\"\\nüéÅ FREE TIER ($200 credit):\")\n",
    "    print(f\"   ‚Ä¢ Covers approximately {free_days:.1f} days\")\n",
    "    \n",
    "    return {\n",
    "        'daily_cost': daily_cost,\n",
    "        'weekly_cost': weekly_cost,\n",
    "        'monthly_cost': monthly_cost,\n",
    "        'free_days': free_days\n",
    "    }\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# costs = estimate_api_costs(collections_per_day=96)\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use estimate_api_costs(collections_per_day=96) to calculate costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e89b4",
   "metadata": {},
   "source": [
    "### Check Collection Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_logs(last_n_lines=50):\n",
    "    \"\"\"Check most recent collection logs\"\"\"\n",
    "    log_file = PROJECT_ROOT / 'logs' / 'collection.log'\n",
    "    \n",
    "    if not log_file.exists():\n",
    "        print(\"‚ÑπÔ∏è No logs yet. Logs will be created after first scheduled run.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã LAST {last_n_lines} LOG LINES:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    with open(log_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-last_n_lines:]:\n",
    "            print(line.rstrip())\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# check_logs(last_n_lines=50)\n",
    "\n",
    "print(\"‚ÑπÔ∏è Use check_logs(last_n_lines=50) to view recent logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa935355",
   "metadata": {},
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ Quick Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83443ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ QUICK ACTIONS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. Test Collection (5 edges):\n",
    "   run_collection_test()\n",
    "\n",
    "2. Full Collection (all edges):\n",
    "   run_collection_full()\n",
    "\n",
    "3. Validate Data:\n",
    "   validate_collection()\n",
    "\n",
    "4. Plot Node Map:\n",
    "   plot_node_coverage()\n",
    "\n",
    "5. Plot Speed Distribution:\n",
    "   plot_speed_distribution()\n",
    "\n",
    "6. Refresh Topology:\n",
    "   refresh_topology(force=True)\n",
    "\n",
    "7. Update Config:\n",
    "   update_config('collection', 'batch_size', 10)\n",
    "\n",
    "8. Estimate Costs:\n",
    "   estimate_api_costs(collections_per_day=96)\n",
    "\n",
    "9. Create Deployment Package:\n",
    "   create_deployment_package()\n",
    "\n",
    "10. Create Cron Script:\n",
    "    create_cron_script()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8de7e4",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Notes\n",
    "\n",
    "### Collection Schedule Recommendations\n",
    "\n",
    "**For Academic Project (7-14 days):**\n",
    "- **Frequency:** Every 15 minutes (96 collections/day)\n",
    "- **Coverage:** Peak hours (7-9 AM, 5-7 PM) + off-peak\n",
    "- **Duration:** 7-14 days minimum for temporal patterns\n",
    "- **Cost:** ~$11-22 with free $200 credit\n",
    "\n",
    "**For Production:**\n",
    "- **Frequency:** Every 5-10 minutes during peak hours\n",
    "- **Coverage:** 24/7 with adaptive intervals\n",
    "- **Duration:** Continuous with data retention policy\n",
    "\n",
    "### Data Quality Checklist\n",
    "\n",
    "- [x] ‚úÖ Topology cached with 78 nodes\n",
    "- [x] ‚úÖ Min distance 200m between nodes\n",
    "- [x] ‚úÖ Coverage radius 4096m\n",
    "- [x] ‚úÖ Real API only (no mock data)\n",
    "- [ ] üîÑ Collect for 7+ days\n",
    "- [ ] üîÑ Validate completeness\n",
    "- [ ] üîÑ Check for missing timestamps\n",
    "- [ ] üîÑ Monitor API errors\n",
    "\n",
    "### Deployment Checklist\n",
    "\n",
    "- [ ] üöÄ Create GCP VM instance\n",
    "- [ ] üöÄ Install conda environment\n",
    "- [ ] üöÄ Copy deployment package\n",
    "- [ ] üöÄ Configure .env with API key\n",
    "- [ ] üöÄ Test collection manually\n",
    "- [ ] üöÄ Setup cron schedule\n",
    "- [ ] üöÄ Monitor logs daily\n",
    "- [ ] üöÄ Track API costs\n",
    "\n",
    "---\n",
    "**End of Control Panel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69cc53",
   "metadata": {},
   "source": [
    "# üéõÔ∏è TRAFFIC FORECAST - CONTROL PANEL v5.0\n",
    "\n",
    "**Project:** Traffic Forecast Academic (DSP391m)  \n",
    "**Version:** 5.0 - Real API Only  \n",
    "**Coverage:** HCMC Downtown, 4096m radius, 78 nodes  \n",
    "**Updated:** October 29, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Pipeline Overview\n",
    "\n",
    "This notebook controls the **entire data collection pipeline** for deployment:\n",
    "\n",
    "1. **Configuration Management** - Update configs, verify settings\n",
    "2. **Node Topology** - Manage Overpass cache, view node distribution\n",
    "3. **Data Collection** - Run Google API collection, monitor progress\n",
    "4. **Data Quality** - Validate collected data, check completeness\n",
    "5. **Visualization** - Plot traffic patterns, node coverage\n",
    "6. **Deployment** - Deploy to GCP VM, schedule collection\n",
    "7. **Monitoring** - Check logs, API costs, system health\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa2e41",
   "metadata": {},
   "source": [
    "## 1. Project Overview & Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0642c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from traffic_forecast import PROJECT_ROOT, __version__\n",
    "from traffic_forecast.ml.data_loader import DataLoader\n",
    "\n",
    "print(f'Project Root: {PROJECT_ROOT}')\n",
    "print(f'Version: {__version__}')\n",
    "print(f'Python: {sys.version.split()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d783c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system status\n",
    "loader = DataLoader()\n",
    "runs = loader.list_runs()\n",
    "\n",
    "print('SYSTEM STATUS')\n",
    "print('=' * 60)\n",
    "print(f'Total Data Runs: {len(runs)}')\n",
    "print(f'Latest Run: {runs[0] if runs else \"None\"}')\n",
    "\n",
    "# Check models availability\n",
    "try:\n",
    "    from traffic_forecast.ml.trainer import ModelTrainer\n",
    "    print('ML Models: READY')\n",
    "except Exception as e:\n",
    "    print(f'ML Models: ERROR - {e}')\n",
    "\n",
    "try:\n",
    "    from traffic_forecast.models.lstm_traffic import LSTMTrafficPredictor\n",
    "    print('LSTM Model: AVAILABLE')\n",
    "except Exception as e:\n",
    "    print(f'LSTM Model: NOT AVAILABLE')\n",
    "\n",
    "try:\n",
    "    from traffic_forecast.models.graph import ASTGCNTrafficModel\n",
    "    print('ASTGCN Model: AVAILABLE')\n",
    "except Exception as e:\n",
    "    print(f'ASTGCN Model: NOT AVAILABLE')\n",
    "\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba6e77",
   "metadata": {},
   "source": [
    "## 2. Quick Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcffb5",
   "metadata": {},
   "source": [
    "### 2.1 Run Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76504f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single collection cycle using subprocess\n",
    "import subprocess\n",
    "\n",
    "collection_script = PROJECT_ROOT / 'scripts' / 'collection' / 'collect_and_render.py'\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['conda', 'run', '-n', 'dsp', 'python', str(collection_script), '--once', '--no-visualize'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print('ERROR:', result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090f98c",
   "metadata": {},
   "source": [
    "### 2.2 Train ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f18fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick model training\n",
    "from traffic_forecast.ml.trainer import ModelTrainer\n",
    "from traffic_forecast.ml.data_loader import DataLoader\n",
    "\n",
    "loader = DataLoader()\n",
    "X_train, X_test, y_train, y_test = loader.prepare_train_test()\n",
    "\n",
    "# Train XGBoost (best model)\n",
    "trainer = ModelTrainer(model_type='xgboost')\n",
    "trainer.train(X_train, y_train)\n",
    "metrics = trainer.evaluate(X_test, y_test)\n",
    "\n",
    "print('\\nXGBoost Training Results:')\n",
    "for metric, value in metrics.items():\n",
    "    print(f'{metric}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da368f63",
   "metadata": {},
   "source": [
    "## 3. Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf086a",
   "metadata": {},
   "source": [
    "### 3.1 List All Data Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "runs = loader.list_runs()\n",
    "run_info = []\n",
    "\n",
    "for run_dir in runs[:10]:  # Show latest 10\n",
    "    run_info.append({\n",
    "        'Run Directory': run_dir.name,\n",
    "        'Date': run_dir.stat().st_mtime\n",
    "    })\n",
    "\n",
    "df_runs = pd.DataFrame(run_info)\n",
    "df_runs['Date'] = pd.to_datetime(df_runs['Date'], unit='s')\n",
    "df_runs = df_runs.sort_values('Date', ascending=False)\n",
    "\n",
    "print(f'Latest {len(df_runs)} Data Runs:')\n",
    "display(df_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194000cd",
   "metadata": {},
   "source": [
    "### 3.2 Cleanup Old Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08351be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup runs older than 14 days using subprocess\n",
    "import subprocess\n",
    "\n",
    "cleanup_script = PROJECT_ROOT / 'scripts' / 'data_management' / 'cleanup_runs.py'\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['conda', 'run', '-n', 'dsp', 'python', str(cleanup_script), '--days', '14'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print('ERROR:', result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0caf6a",
   "metadata": {},
   "source": [
    "## 4. Model Training Center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16794a",
   "metadata": {},
   "source": [
    "### 4.1 Train All Traditional ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic_forecast.ml.trainer import ModelTrainer\n",
    "\n",
    "models_to_train = ['random_forest', 'xgboost', 'lightgbm', 'gradient_boosting']\n",
    "results = {}\n",
    "\n",
    "for model_type in models_to_train:\n",
    "    print(f'\\nTraining {model_type}...')\n",
    "    trainer = ModelTrainer(model_type=model_type)\n",
    "    trainer.train(X_train, y_train)\n",
    "    metrics = trainer.evaluate(X_test, y_test)\n",
    "    results[model_type] = metrics\n",
    "    \n",
    "# Display comparison\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results = df_results.sort_values('r2', ascending=False)\n",
    "\n",
    "print('\\nModel Comparison:')\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a6d90",
   "metadata": {},
   "source": [
    "### 4.2 Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b670628",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from traffic_forecast.models.lstm_traffic import LSTMTrafficPredictor\n",
    "    \n",
    "    lstm = LSTMTrafficPredictor(\n",
    "        sequence_length=12,\n",
    "        lstm_units=[128, 64],\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    \n",
    "    history = lstm.fit(\n",
    "        X_train, y_train,\n",
    "        X_val=X_test, y_val=y_test,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    metrics = lstm.evaluate(X_test, y_test)\n",
    "    print('\\nLSTM Results:')\n",
    "    for k, v in metrics.items():\n",
    "        print(f'{k}: {v:.4f}')\n",
    "except Exception as e:\n",
    "    print(f'LSTM training failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828808e3",
   "metadata": {},
   "source": [
    "## 5. Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38898e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load latest data\n",
    "df_latest = loader.load_latest_run()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Speed distribution\n",
    "axes[0, 0].hist(df_latest['speed_kmh'], bins=50, edgecolor='black')\n",
    "axes[0, 0].set_title('Speed Distribution')\n",
    "axes[0, 0].set_xlabel('Speed (km/h)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Duration distribution\n",
    "axes[0, 1].hist(df_latest['duration_minutes'], bins=50, edgecolor='black', color='orange')\n",
    "axes[0, 1].set_title('Duration Distribution')\n",
    "axes[0, 1].set_xlabel('Duration (minutes)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Weather conditions\n",
    "if 'temperature_c' in df_latest.columns:\n",
    "    axes[1, 0].scatter(df_latest['temperature_c'], df_latest['speed_kmh'], alpha=0.3)\n",
    "    axes[1, 0].set_title('Temperature vs Speed')\n",
    "    axes[1, 0].set_xlabel('Temperature (C)')\n",
    "    axes[1, 0].set_ylabel('Speed (km/h)')\n",
    "\n",
    "# Time series\n",
    "if 'timestamp' in df_latest.columns:\n",
    "    speed_ts = df_latest.groupby('timestamp')['speed_kmh'].mean()\n",
    "    axes[1, 1].plot(speed_ts.index, speed_ts.values)\n",
    "    axes[1, 1].set_title('Average Speed Over Time')\n",
    "    axes[1, 1].set_xlabel('Time')\n",
    "    axes[1, 1].set_ylabel('Speed (km/h)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43fab0",
   "metadata": {},
   "source": [
    "## 6. System Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_module(module_name):\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        return 'OK'\n",
    "    except:\n",
    "        return 'MISSING'\n",
    "\n",
    "health_status = {\n",
    "    'Module': ['pandas', 'numpy', 'scikit-learn', 'xgboost', 'lightgbm', 'tensorflow'],\n",
    "    'Status': []\n",
    "}\n",
    "\n",
    "for module in health_status['Module']:\n",
    "    health_status['Status'].append(check_module(module))\n",
    "\n",
    "df_health = pd.DataFrame(health_status)\n",
    "\n",
    "print('System Health Check:')\n",
    "display(df_health)\n",
    "\n",
    "# Check disk space\n",
    "data_dir = PROJECT_ROOT / 'data' / 'downloads'\n",
    "if data_dir.exists():\n",
    "    total_size = sum(f.stat().st_size for f in data_dir.rglob('*') if f.is_file())\n",
    "    print(f'\\nData Directory Size: {total_size / 1024 / 1024:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e00788",
   "metadata": {},
   "source": [
    "## 7. Deployment & Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c723c",
   "metadata": {},
   "source": [
    "### 7.1 Export Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best model for deployment\n",
    "best_trainer = ModelTrainer(model_type='xgboost')\n",
    "best_trainer.train(X_train, y_train)\n",
    "\n",
    "export_dir = PROJECT_ROOT / 'models' / 'production'\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = export_dir / 'xgboost_best.pkl'\n",
    "best_trainer.save(str(model_path))\n",
    "print(f'Model exported to: {export_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167c664",
   "metadata": {},
   "source": [
    "### 7.2 Run API Server (Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adf40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start FastAPI server (run this in a separate terminal)\n",
    "# conda run -n dsp uvicorn traffic_forecast.api.main:app --reload --port 8000\n",
    "\n",
    "print('To start the API server, run this command in a terminal:')\n",
    "print('conda run -n dsp uvicorn traffic_forecast.api.main:app --reload --port 8000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f477c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## End of Control Dashboard\n",
    "\n",
    "**Navigation:**\n",
    "- [DATA_DASHBOARD.ipynb](./DATA_DASHBOARD.ipynb) - Data exploration and analysis\n",
    "- [ML_TRAINING.ipynb](./ML_TRAINING.ipynb) - Detailed ML training\n",
    "- [SCRIPTS_RUNNER.ipynb](./SCRIPTS_RUNNER.ipynb) - Script execution\n",
    "\n",
    "**Documentation:**\n",
    "- [README.md](../README.md) - Project overview\n",
    "- [CHANGELOG.md](../CHANGELOG.md) - Version history"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
