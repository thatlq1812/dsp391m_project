\documentclass[12pt,a4paper]{article}

% Font encoding for pdfLaTeX
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{times}

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{pifont}

% Define checkmark and xmark commands
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{listings}
\usepackage{float}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{setspace}

% Header and Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={STMGT Traffic Forecasting - HCMC},
    pdfauthor={THAT Le Quang, HUNG Le Minh, TOAN Nguyen Quy},
}

% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  language=Python
}

\begin{document}

% ============================================================================
% TITLE PAGE
% ============================================================================
\begin{titlepage}
\centering

\vspace*{1cm}

% University logo
\includegraphics[width=0.5\textwidth]{figures/fig_00_fpt_logo.png}

\vspace{1cm}

{\Large \textbf{FPT UNIVERSITY}}\\
\vspace{0.5cm}
{\large FACULTY OF ARTIFICIAL INTELLIGENCE}\\

\vspace{2cm}

{\huge \textbf{Data Science Capstone Project Final Report}}\\
{\large DSP391m - Fall 2025}\\

\vspace{1.5cm}

{\LARGE \textbf{Multi-Modal Spatio-Temporal Graph}}\\
\vspace{0.3cm}
{\LARGE \textbf{Transformer for Real-Time Traffic}}\\
\vspace{0.3cm}
{\LARGE \textbf{Speed Forecasting in Ho Chi Minh City}}\\

\vspace{2cm}

{\large \textbf{Team Members:}}\\
\vspace{0.5cm}
\begin{tabular}{ll}
1. & HUNG Le Minh - SE182706 \\
2. & TOAN Nguyen Quy - SE182785 \\
3. & THAT Le Quang - SE183256 \\
\end{tabular}\\

\vspace{0.5cm}
{\large \textbf{Major:} AI \& Data Science}\\

\vspace{1cm}

{\large \textbf{Instructor:} TRUNG Nguyen Quoc - TrungNQ46}\\

\vfill

{\large Ho Chi Minh City, November 2025}

\end{titlepage}

% ============================================================================
% ABSTRACT PAGE
% ============================================================================
\newpage
\thispagestyle{empty}
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

\noindent
Traffic congestion in rapidly urbanizing cities like Ho Chi Minh City poses significant economic and environmental challenges, costing approximately \$1.2 billion USD annually. This research presents STMGT (Spatio-Temporal Multi-Modal Graph Transformer), a novel deep learning architecture for accurate traffic speed forecasting. Our approach combines Graph Attention Networks (GATv2) for spatial modeling, Transformers for temporal dependencies, and cross-attention mechanisms for weather integration. The model outputs probabilistic predictions using Gaussian Mixture Models (GMM) to quantify uncertainty.

Evaluated on 29 days of real-world data from 62 intersections in Ho Chi Minh City, STMGT achieves MAE of 3.08 km/h and $R^2$ of 0.82, outperforming baseline models (LSTM, GCN, GraphWaveNet) by 22--36\%. The system is deployed as a production-ready FastAPI service with sub-400ms inference latency. Our contributions include: (1) a parallel spatio-temporal processing architecture validated through ablation studies, (2) weather-aware cross-attention mechanism improving accuracy by 12\%, and (3) comprehensive benchmarking against established baselines. The system demonstrates practical applicability for intelligent transportation systems, enabling proactive traffic management and route optimization.

\vspace{0.5cm}

\noindent\textbf{Keywords:} Traffic forecasting, graph neural networks, transformers, spatio-temporal modeling, attention mechanisms, Gaussian mixture models, intelligent transportation systems, urban computing, Ho Chi Minh City

% ============================================================================
% ACKNOWLEDGMENTS
% ============================================================================
\newpage
\section*{Acknowledgments}
\addcontentsline{toc}{section}{Acknowledgments}

We would like to express our sincere gratitude to all those who have supported us throughout this research project.

First and foremost, we are deeply grateful to our instructor TRUNG Nguyen Quoc for their invaluable guidance, continuous support, and insightful feedback throughout the development of this project. Their expertise in machine learning and transportation systems has been instrumental in shaping this research.

We would like to thank FPT University and the Faculty of Artificial Intelligence for providing the resources and academic environment that made this research possible. Special thanks to the teaching staff and fellow students who provided constructive discussions and suggestions during the development phase.

We are grateful to the OpenStreetMap community and Google Maps API for providing the geographic and traffic data that enabled this research. Thanks also to the open-source community for the excellent tools and frameworks (PyTorch, PyTorch Geometric, FastAPI) that formed the foundation of this implementation.

Finally, we would like to thank our families and friends for their unwavering support and encouragement throughout our studies and this project.

\vspace{1cm}

\noindent Ho Chi Minh City, November 2025\\
\noindent On behalf of the team,\\
\noindent THAT Le Quang, HUNG Le Minh, TOAN Nguyen Quy

% ============================================================================
% TABLE OF CONTENTS
% ============================================================================
\newpage
\tableofcontents

\newpage
\listoffigures

\newpage
\listoftables

% ============================================================================
% MAIN CONTENT - Using \input{} to include sections
% ============================================================================
\newpage
\setcounter{page}{1}
\onehalfspacing

% Section 1: Introduction
\input{sections/01_introduction}

% Section 2: Literature Review  
\input{sections/02_literature_review}

% Section 3: Data Description
\input{sections/03_data_description}

% Section 4: Data Preprocessing
\input{sections/04_data_preprocessing}

% Section 5: Exploratory Data Analysis
\input{sections/05_eda}

% Section 6: Methodology
\input{sections/06_methodology}

% Section 7: Model Development
\input{sections/07_model_development}

% Section 8: Evaluation and Tuning
\input{sections/08_evaluation}

% Section 9: Results and Visualization
\input{sections/09_results}

% Section 10: Conclusion
\input{sections/10_conclusion}

% ============================================================================
% REFERENCES
% ============================================================================

\begin{thebibliography}{99}

\bibitem{hochreiter1997long}
S. Hochreiter and J. Schmidhuber,
``Long Short-Term Memory,''
\textit{Neural Computation}, vol. 9, no. 8, pp. 1735--1780, 1997.

\bibitem{bishop1994mixture}
C. M. Bishop,
``Mixture Density Networks,''
Neural Computing Research Group Report, Aston University, 1994.

\bibitem{gneiting2007strictly}
T. Gneiting and A. E. Raftery,
``Strictly Proper Scoring Rules, Prediction, and Estimation,''
\textit{Journal of the American Statistical Association}, vol. 102, no. 477, pp. 359--378, 2007.

\bibitem{defferrard2016convolutional}
M. Defferrard, X. Bresson, and P. Vandergheynst,
``Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,''
in \textit{Advances in Neural Information Processing Systems 29 (NeurIPS)}, 2016, pp. 3844--3852.

\bibitem{kipf2017semi}
T. N. Kipf and M. Welling,
``Semi-Supervised Classification with Graph Convolutional Networks,''
in \textit{International Conference on Learning Representations (ICLR)}, 2017.

\bibitem{velickovic2018graph}
P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio,
``Graph Attention Networks,''
in \textit{International Conference on Learning Representations (ICLR)}, 2018.

\bibitem{brody2022attentive}
S. Brody, U. Alon, and E. Yahav,
``How Attentive are Graph Attention Networks?''
in \textit{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{vaswani2017attention}
A. Vaswani \textit{et al.},
``Attention Is All You Need,''
in \textit{Advances in Neural Information Processing Systems 30 (NeurIPS)}, 2017, pp. 5998--6008.

\bibitem{lim2021temporal}
B. Lim, S. Ö. Arık, N. Loeff, and T. Pfister,
``Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting,''
\textit{International Journal of Forecasting}, vol. 37, no. 4, pp. 1748--1760, 2021.

\bibitem{li2018dcrnn}
Y. Li, R. Yu, C. Shahabi, and Y. Liu,
``Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,''
in \textit{International Conference on Learning Representations (ICLR)}, 2018.

\bibitem{yu2018spatio}
B. Yu, H. Yin, and Z. Zhu,
``Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting,''
in \textit{Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI)}, 2018, pp. 3777--3783.

\bibitem{guo2019attention}
S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan,
``Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting,''
in \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, vol. 33, no. 01, 2019, pp. 922--929.

\bibitem{wu2019graph}
Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang,
``Graph WaveNet for Deep Spatial-Temporal Graph Modeling,''
in \textit{Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)}, 2019, pp. 1907--1913.

\bibitem{wu2020connecting}
Z. Wu \textit{et al.},
``Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks,''
in \textit{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, 2020, pp. 753--763.

\bibitem{box2015time}
G. E. P. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung,
\textit{Time Series Analysis: Forecasting and Control}, 5th ed. Wiley, 2015.

\bibitem{hastie2009elements}
T. Hastie, R. Tibshirani, and J. Friedman,
\textit{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}, 2nd ed. Springer, 2009.

\bibitem{fey2019fast}
M. Fey and J. E. Lenssen,
``Fast Graph Representation Learning with PyTorch Geometric,''
in \textit{ICLR 2019 Workshop on Representation Learning on Graphs and Manifolds}, 2019.

\bibitem{li2020mtgnn}
Z. Li \textit{et al.},
``Multivariate Time Series Forecasting with Transfer Entropy Graph,''
\textit{arXiv preprint arXiv:2005.01185}, 2020.

\bibitem{bishop2006prml}
C. M. Bishop,
\textit{Pattern Recognition and Machine Learning}.
Springer, 2006.

\bibitem{liu2023uncertain}
Y. Liu, J. Wang, and X. Chen,
``Uncertainty Quantification in Deep Learning for Traffic Forecasting,''
\textit{IEEE Transactions on Intelligent Transportation Systems}, vol. 24, no. 3, pp. 2451--2463, 2023.

\bibitem{brody2022gatv2}
S. Brody, U. Alon, and E. Yahav,
``How Attentive are Graph Attention Networks?''
in \textit{International Conference on Learning Representations (ICLR)}, 2022.

\end{thebibliography}

% ============================================================================
% APPENDICES
% ============================================================================
\newpage
\appendix

\section{Model Configuration}
\label{app:config}

\subsection{STMGT Hyperparameters}

\begin{table}[H]
\centering
\caption{STMGT Model Hyperparameters}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Hidden Dimension & 96 \\
Number of STMGT Blocks & 3 \\
GATv2 Attention Heads & 4 \\
Transformer Attention Heads & 4 \\
Dropout Rate & 0.2 \\
DropEdge Rate & 0.1 \\
Learning Rate & 0.001 \\
Batch Size & 32 \\
Max Epochs & 50 \\
Early Stopping Patience & 10 \\
Optimizer & Adam \\
Weight Decay & 1e-5 \\
GMM Components (K) & 5 \\
Input Sequence Length & 12 (3 hours) \\
Prediction Horizon & 12 (3 hours) \\
Total Parameters & 680,448 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Configuration}

\begin{lstlisting}[caption={Training Configuration (YAML)},language=Python]
model:
  hidden_dim: 96
  num_blocks: 3
  gat_heads: 4
  transformer_heads: 4
  dropout: 0.2
  dropedge: 0.1
  gmm_components: 5

training:
  learning_rate: 0.001
  batch_size: 32
  max_epochs: 50
  early_stopping_patience: 10
  weight_decay: 1.0e-5
  gradient_clip_val: 1.0
  lr_scheduler:
    type: ReduceLROnPlateau
    mode: min
    factor: 0.5
    patience: 5
    min_lr: 1.0e-6

data:
  sequence_length: 12
  prediction_horizon: 12
  train_ratio: 0.70
  val_ratio: 0.15
  test_ratio: 0.15
  normalize: true
  augmentation: false
\end{lstlisting}

\section{Additional Results}
\label{app:results}

\subsection{Detailed Performance Metrics}

\begin{table}[H]
\centering
\caption{Complete Performance Comparison Across All Models}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{MAE} & \textbf{RMSE} & \textbf{$R^2$} & \textbf{MAPE} & \textbf{CRPS} & \textbf{Params} \\
& \textbf{(km/h)} & \textbf{(km/h)} & & & & \\
\midrule
LSTM & 4.42--4.85 & 6.08--6.23 & 0.185--0.64 & 20.62--28.91\% & -- & $\sim$800K \\
GCN & 3.91 & $\sim$5.0 & $\sim$0.72 & $\sim$25\% & -- & 340K \\
GraphWaveNet & 3.95 & 5.12 & 0.71 & 24.58\% & -- & $\sim$600K \\
\textbf{STMGT V2} & \textbf{3.08} & \textbf{4.53} & \textbf{0.82} & \textbf{19.26\%} & \textbf{2.23} & 680K \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance by Time Horizon}

\begin{table}[H]
\centering
\caption{STMGT Performance Across Prediction Horizons}
\begin{tabular}{lccc}
\toprule
\textbf{Horizon} & \textbf{MAE (km/h)} & \textbf{RMSE (km/h)} & \textbf{$R^2$} \\
\midrule
15 min (t+1) & 2.85 & 4.21 & 0.84 \\
30 min (t+2) & 2.94 & 4.35 & 0.83 \\
1 hour (t+4) & 3.08 & 4.53 & 0.82 \\
2 hours (t+8) & 3.52 & 5.02 & 0.78 \\
3 hours (t+12) & 4.15 & 5.68 & 0.73 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study Results}

\begin{table}[H]
\centering
\caption{Ablation Study: Component Contribution}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{MAE} & \textbf{RMSE} & \textbf{$R^2$} & \textbf{$\Delta$ MAE} \\
\midrule
Full STMGT & \textbf{3.08} & \textbf{4.53} & \textbf{0.82} & baseline \\
w/o Weather Cross-Attn & 3.45 & 4.89 & 0.78 & +15.0\% \\
w/o Gated Fusion & 3.68 & 5.12 & 0.75 & +19.5\% \\
Sequential (not parallel) & 3.51 & 4.95 & 0.77 & +14.0\% \\
w/o GMM (MSE loss) & 3.28 & 4.67 & 0.80 & +6.5\% \\
Single STMGT block & 3.42 & 4.82 & 0.79 & +11.0\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Code Repository and Reproducibility}
\label{app:code}

The complete source code, trained models, datasets, and documentation are publicly available at:

\begin{center}
\url{https://github.com/thatlq1812/dsp391m_project}
\end{center}

\subsection{Repository Structure}

\begin{itemize}
\item \texttt{traffic\_forecast/}: Core model implementation
  \begin{itemize}
  \item \texttt{core/}: STMGT architecture (GATv2, Transformer, Cross-Attention)
  \item \texttt{augmentation/}: Data augmentation pipelines
  \item \texttt{collectors/}: Real-time data collection scripts
  \end{itemize}
\item \texttt{scripts/}: Training, evaluation, and visualization scripts
  \begin{itemize}
  \item \texttt{training/}: Model training workflows
  \item \texttt{visualization/}: Figure generation (20 figures)
  \item \texttt{data/}: Data preprocessing and generation
  \end{itemize}
\item \texttt{configs/}: Model and training configurations (YAML/JSON)
\item \texttt{data/}: Dataset storage
  \begin{itemize}
  \item \texttt{processed/}: Preprocessed parquet files
  \item \texttt{runs/}: Individual data collection runs
  \end{itemize}
\item \texttt{docs/}: Complete documentation
  \begin{itemize}
  \item \texttt{05\_final\_report/}: This LaTeX report
  \item \texttt{API.md}, \texttt{ARCHITECTURE.md}, \texttt{CLI.md}
  \end{itemize}
\item \texttt{traffic\_api/}: REST API implementation (FastAPI)
\item \texttt{tests/}: Comprehensive test suite (pytest)
\end{itemize}

\subsection{Installation and Usage}

\textbf{Requirements:}
\begin{itemize}
\item Python 3.9+
\item PyTorch 2.0+
\item PyTorch Geometric 2.3+
\item CUDA 11.8+ (optional, for GPU acceleration)
\end{itemize}

\textbf{Quick Start:}
\begin{lstlisting}[language=bash]
# Clone repository
git clone https://github.com/thatlq1812/dsp391m_project
cd dsp391m_project

# Install dependencies
pip install -e .

# Train STMGT model
python scripts/training/train_stmgt.py \
  --config configs/train_normalized_v3.json \
  --output outputs/my_run

# Run API server
bash scripts/run_api.sh

# Generate all figures
python scripts/visualization/generate_all_figures.py
\end{lstlisting}

\subsection{Reproducibility Checklist}

\begin{itemize}
\item[$\checkmark$] Source code publicly available
\item[$\checkmark$] Dependency versions pinned (\texttt{requirements.txt})
\item[$\checkmark$] Random seeds fixed (42 for training)
\item[$\checkmark$] Dataset preprocessing scripts included
\item[$\checkmark$] Model checkpoint saved (\texttt{best\_model.pt})
\item[$\checkmark$] Hyperparameters documented (JSON/YAML)
\item[$\checkmark$] Training logs preserved (\texttt{tensorboard})
\item[$\checkmark$] Evaluation metrics scripts provided
\item[$\checkmark$] Figure generation automated
\end{itemize}

\end{document}
